\chapter{Parameterized algorithm for phasing pedigrees}
Read-based phasing deduces the haplotypes of an individual from sequencing reads that cover multiple variants, while genetic phasing takes only genotypes as input and applies the rules of Mendelian inheritance to infer haplotypes within a pedigree of individuals.
Combining both into an approach that uses these two independent sources of information -- reads and pedigree -- has the potential to deliver results better than each individually.\\

In this chapter, we provide a theoretical framework combining read-based phasing with genetic haplotyping, and describe a fixed-parameter algorithm and its implementation for finding an optimal solution.
We show that leveraging reads of related individuals jointly in this way yields more phased variants and at a higher accuracy than when phased separately, both in simulated and real data.
Coverages as low as 2$\times$ for each member of a trio yield haplotypes that are as accurate as when analyzed separately at 15$\times$ coverage per individual.

\section{Introduction}\label{sec:intro}
With sequencing cost decreasing at an exponential rate, it has now become possible to sequence pedigree of genomes.
As a result, the sequencing datasets from pedigrees are becoming publicly available.
Thus to come up with an algorithm, that solves the haplotyping problem for pedigrees in a joint framework by considering both information sources, from sequencing reads and principles of Mendelian inheritance is very important.

Over the last few years, there has been considerable efforts devoted to phasing using reference panels, phasing pedigrees using genotype data and molecular haplotyping using NGS data.
Broadly, the methods for phasing are classified into three classes. 
First, haplotypes can be inferred from genotype information of large cohorts based on the idea that common ancestry gives rise to shared haplotype tracts, as reviewed by \cite{Browning2011, loh2016fast, loh2016reference}.
This approach is known as \emph{statistical} or \emph{population-based phasing}.
The idea is to explain the genotypes of the target genome by finding the maximum likelihood paths from the reference panel (large set of haplotypes from the population).
It can be applied to unrelated individuals and only requires genotype data, which can be measured at low cost.
While very powerful for common variants, this technique is less accurate for phasing rare variants and cannot be applied at all to private or \textit{de novo} variants.
%
Second, haplotypes can be determined based on genotype data of related individuals, known as \emph{genetic haplotyping} \citep{Glusman2014}.
To solve the phasing problem, one seeks to explain the observed genotypes under the constraints imposed by the Mendelian laws of inheritance, while being parsimonious in terms of recombination events.
For larger pedigrees, such as parents with many children, this approach yields highly accurate phasing \citep{Roach2011, abecasis2002merlin, williams2010rapid}.
On the other hand, it is less accurate for single mother-father-child trios and has the intrinsic limitation of not being able to phase variants that are heterozygous in all individuals.
%
Third, the sequences of the two haplotypes can be determined experimentally, called \emph{molecular haplotyping}.
Many techniques do not resolve the full-length haplotypes but yield blocks of varying sizes.
Approaches furthermore largely differ in the amount of work, DNA, and money they require.
As discussed in Chapter~\ref{ref:chp1}, on one end of the scale, next-generation sequencing (NGS) instruments generate local phase information of the length of a sequenced fragment at ever-decreasing costs.
On the other end, upcoming long-read technologies such as Pacific Biosciences and Oxford Nanopore technologies produce long reads in the order of magnitude of kilo-bases.
The sequencing cost of these technologies is decreasing at an exponential rate.
There is a lot of literature on several computational approaches that utilize data from these technologies to produce haplotypes \citep{rhee2016survey}.
As discussed in Chapter~\ref{ref:chp2}, WhatsHap is an efficient parameterized approach for phasing a single individual.
It basically uses long read data from PacBio technology and performs the Minimum Error Correction of reads in a haplotype aware manner, to produce two haplotypes.

Another sequencing approach consists in breaking both homologous chromosomes into (larger) fragments and 
separating them into a number of pools such that each pool is unlikely to contain fragments from the same locus of both haplotypes.
This can, for instance, be achieved by dilution followed by bar-coded short-read sequencing.
To achieve molecular haplotyping over the range of a full chromosome, 
protocols have been invented to physically separate the two homologous chromosomes, for example by microscopy-based chromosome isolation, fluorescence-activated sorting, or microfluidics-based sorting.
These and other experimental techniques for molecular haplotyping have been surveyed by \cite{Snyder2015}.
They are of great interest because they facilitate phasing of rare variants for single individuals.
Rare variants have been postulated to contribute considerably to clinical traits and are hence of major interest.

\paragraph{Hybrid Approaches.}
The ideas underlying population-based phasing, genetic haplotyping, and read-based phasing have been combined in many ways to create hybrid methods.
\cite{Delaneau2013a}, for instance, use local phase information provided by sequencing reads to enhance their population-based phasing approach SHAPEIT.
Exploiting pedigree information for statistical phasing has also been demonstrated to significantly improve the inferred haplotypes \citep{Marchini2006,CDW13_exact}.
Using their heuristic read-based phasing approach HapCompass, \cite{Aguiar2013} note that combining reads from parent-offspring duos increases performance in regions that are identical by descent (IBD). 
Beyond this approach, we are not aware of prior work to leverage family information towards read-based phasing.
% \paragraph{Haplotype Assembly.}
% When many haplotype fragments are available for one individual, for instance from sequencing, one can attempt to reconstruct the full haplotypes or at least to obtain larger blocks.
% This process is known as \emph{haplotype assembly}, \emph{single-individual haplotyping}, or \emph{read-based phasing} (in case the fragments indeed stem from sequencing reads).
% It requires reads that span two or more heterozygous variants.
% In order to be successful, reads covering as many pairs of consecutive heterozygous variants as possible are desirable.
% At present, third generation sequencing platforms, as marketed by Pacific Biosciences (PacBio) and Oxford Nanopore, become more widespread and offer reads spanning thousands to tens of thousands of nucleotides.
% Although error-rates are much higher than for common second generation technologies, the longer reads provide substantially more phase information and hence render them promising platforms for read-based phasing.
% 
% To formalize the haplotype assembly problem in the face of errors, we define \emph{operations} on the input matrix and ask for the minimum number of operations one needs to apply to render it feasible.
% Different such operations have been studied, in particular removal of rows, resulting in the \emph{Minimum Fragment Removal (MFR)} problem, removal of columns, resulting in the \emph{Minimum SNP Removal (MSR)} problem, and flipping of bits, resulting in the \emph{Minimum Error Correction (MEC)} problem.
% All three problems are NP-hard \citep{Lancia2001,Cilibrasi2007}.
% Flipping of bits corresponds to correcting sequencing errors and hence the MEC problem has received most attention in the literature and is most relevant in practice.
% A wealth of exact and heuristic approaches to solve the MEC problem exists.
% Exact approaches, which solve the problem optimally, include integer linear programming \citep{Fouilhoux2012,CDW13_exact}, and fixed-parameter tractable (FPT) algorithms \citep{he2010optimal,Patterson2015,Pirola2015}.
% Refer to the reviews by \cite{Schwartz2010} and \cite{Rhee2015} for further related approaches.

% \begin{figure}[t!]\centering
% \includegraphics[width=\columnwidth]{{Marschall.152.fig.1}.pdf}
% \caption{Seven SNP loci covered by reads (horizontal bars) in three individuals.
% Unphased genotypes are indicated by labels 0/0, 0/1, and 1/1.
% The alleles that a read supports are printed in white.}
% \label{fig:ex_pedigree}
% \end{figure}
\paragraph{Contributions.}
Here, we build upon our previous approach WhatsHap \citep{Patterson2014,Patterson2015} and generalize it to jointly handle sequencing reads of related individuals.
WhatsHap is an FPT approach that solves the (weighted) MEC problem optimally, exponentially in time with maximum coverage, but linearly in the number of variants.
In particular the run-time does not explicitly depend on the read length.
These properties make it particularly apt for current long-read data. 
This has also been observed by \citet{Kuleshov2014b}, who approached the weighted MEC problem in a message-passing framework and, by doing so, independently arrived at the same DP algorithm used in WhatsHap.
The exponential run-time in the maximum coverage does not constitute a problem in practice because reads can be removed in regions of excess coverage without loosing much information.
The evaluation by \cite{Patterson2015} suggests that pruning data to a maximum coverage of 15$\times$ yields excellent results while an even higher coverage does not deliver a significant additional improvement.

Here, we introduce a unifying formal framework to fully integrate read-based and genetic haplotyping.
To this end, we define the Weighted Minimum Error-Correction on Pedigrees Problem, termed PedMEC, which generalizes the (weighted) MEC problem and accounts for Mendelian inheritance and recombination.
This problem is NP-hard.
We generalize the WhatsHap algorithm for solving this problem optimally and thereby show that PedMEC is fixed-parameter tractable.
When the maximum coverage is bounded, the run-time of our algorithm is linear in the number of variants and does not explicitly depend on the read length, hence inheriting the favorable properties of WhatsHap.

We target an application scenario where related individuals are sequenced using error-prone long-read technologies such as PacBio sequencing.
As a driving question motivating this research, we ask how much coverage is needed for resolving haplotypes in related individuals as opposed to single or unrelated individuals.
Our focus is on phasing and we do not consider the genotyping step, which can either be done from the same data or from orthogonal and potentially cheaper data sources such as micro-arrays or short-read sequencing.
On simulated and real PacBio data, we show that sequencing each individual in a mother-father-child trio to 5$\times$ coverage is sufficient to establish a high-quality phasing.
This is in stark contrast to state-of-the-art single-individual read-based phasing, which yields worse results even for 15$\times$ coverage with respect to both error rates and numbers of phased variants.
We furthermore demonstrate that our technique also exhibits favorable properties of genetic haplotyping approaches:
Because of genotype relationships between related individuals, we are able to infer correct phases even \emph{between} haplotype blocks that are \emph{not connected} by any sequencing reads in any of the individuals.

\begin{table*}
\caption{Overview of common notation.}\label{tab:notation}
\begin{tabular}{l@{\hspace{1em}}l@{\hspace{1em}}l}
\hline
Notation & Meaning & Example \\\hline
$\mathcal{I}$ & Set of individuals & $\{1,2,3,4\}$ \\
$\mathcal{T}$ & Set of trio relationships & $\{(1,2,3),(1,2,4)\}$\\[.5em]
$\mathcal{F}_i\in\{0,1,-\}^{R_i\times M}$ & Input SNP matrix for individual $i\in\mathcal{I}$ & $\left[ \begin{array}{p{1em}p{1em}p{1em}p{1em}p{1em}}
- & - & 1 & 0 & 1 \\
0 & 1 & 1 & 1 & - \end{array} \right]$\vspace{.5em}\\
$\mathcal{W}_i\in\N^{R_i\times M}$ & Matrix of weights for individual $i\in\mathcal{I}$ & $\left[ \begin{array}{p{1em}p{1em}p{1em}p{1em}p{1em}}
0 & 0 & 10 & 21 & 7 \\
13 & 9 & 31 & 25 & 0 \end{array} \right]$\vspace{.5em}\\ 
$\mathcal{X}\in\N^M$  & Recombination cost vector & $(5,20,12,23,11)$\\
$g_i\in\{0,1,2\}^M$ & Input genotypes for individual $i$ & $(0,2,2,1,1)$ \\\hline
$A(k)$ & Set of reads active in column $k$ & $\big\{[\,\text{-}\ \text{-}\ 1\ 0\ 1], [0\ 1\ 1\ 1\ \text{-}\,]\big\}$ \\ 
$\Delta_C(k,B,t)$ & Local cost for column $k$, bipartition $B$, & 10 \\
 & and transmission tuple $t$ & \\
$C(k,B,t)$ & DP table entry for column $k$, bipartition $B$, & 37 \\
 & and transmission tuple $t$ & \\\hline
$h_i^0, h_i^1\in\{0,1\}^M$ & Sought haplotypes for individual $i$ & $(0,1,1,1,0), (0,1,1,0,1)$ \\
$t_{m\to c},t_{f\to c}\in\{0,1\}^M$ & Sought transmission vectors for trio $(m,f,c)\in\mathcal{T}$ & $(0,0,0,1,1)$\\
\hline
\end{tabular}
\end{table*}

\section{The Weighted Minimum Error Correction Problem on Pedigrees}
As discussed in Chapter~\ref{ref:chp1}, read-based phasing has predominantly been formulated as the Minimum Error Correction (MEC) problem \citep{Cilibrasi2007} and its weighted sibling wMEC \citep{Greenberg2004}.

In this section, we present a novel formulation for jointly phasing individuals in a pedigree.
To this end, we generalize wMEC (see Problem~\ref{prob:wmec}) to account for multiple individuals in a pedigree simultaneously while modeling inheritance and recombination.
An overview of notation we use is provided in Table~\ref{tab:notation}.
We assume our pedigree to contain a set of $N$ individuals $\mathcal{I}=\{1,\ldots,N\}$.
Relationships between individuals are given as a set of (ordered) mother-father-child triples $\mathcal{T}$.
For example, if $\mathcal{I}=\{1,2,3,4\}$, then $\mathcal{T}=\big\{(1,2,3),(1,2,4)\big\}$ corresponds to a pedigree where individuals $1$ and $2$ are the parents of individuals $3$ and $4$.
% Refer to Figure~\ref{fig:notation} for an illustration.
We only consider non-degenerate cases without circular relationships and where each individual appears as a child in at most one triple.
Furthermore, we assume all considered variants to be non-overlapping and bi-allelic.
Each individual $i$ comes with a \emph{genotype vector} $g_i\in\{0,1,2\}^M$, giving the genotypes of all $M$ variants.
Genotypes $0$, $1$, and $2$ correspond to being homozygous in the reference allele, heterozygous, and homozygous in the alternative allele, respectively.
In the context of phasing, we can restrict ourselves to the set of variants that are heterozygous in at least one of the individuals, that is, to variants $k$ such that $g_i(k)=1$ for at least one individual $i\in\mathcal{I}$.
For each individual $i\in \mathcal{I}$, a number of $R_i$ aligned sequencing reads is provided as input, giving rise to one SNP matrix $\mathcal{F}_i\in\{0,1,-\}^{R_i\times M}$ and one weight matrix $\mathcal{W}_i\in\N^{R_i\times M}$ per individual.
We seek to compute two haplotypes $h^0_i,h^1_i\in\{0,1\}^M$ for all individuals $i\in\mathcal{I}$.
As before in the MEC problem, we want these haplotypes to be consistent with the sequencing reads.

In addition, we want the haplotypes to respect the constraints given by the pedigree.
Recall that in each parent, the two homologous chromosomes recombine during meiosis to give rise to a haploid gamete that is passed on to the offspring.
Therefore, each haplotype of a child should be representable as a mosaic of the two haplotypes of the respective parent with few recombination events.
To control the number of recombination events, we assume a per-site recombination cost of $\mathcal{X}(k)$ to be provided as input.
Controlling the recombination cost per site is important because it is not equally likely to happen at all points along a chromosome.
Instead, \emph{recombination hotspots} exist, where recombination is much more likely to occur (and should hence be penalized less strongly in our model).
The cost $\mathcal{X}(k)$ should be interpreted as the (phred-scaled) probability that a recombination event occurs between variant $k-1$ and variant $k$.
To formalize the inheritance process, we define \emph{transmission vectors} $t_{m\to c}, t_{f\to c}\in \{0,1\}^M$ for each triple $(m,f,c)\in T$.
The values $t_{m\to c}(k)$ and $t_{f\to c}(k)$ tell which allele at site $k$ is transmitted by mother and father, respectively.
The haplotypes we seek to compute have to be \emph{compatible} with transmission vectors, defined formally as follows.

\begin{definition}[Transmission vector compatibility]
For a given trio $(m,f,c)\in\mathcal{T}$, the haplotypes $h^0_m,h^1_m,h^0_f,h^1_f,h^0_c,h^1_c\in\{0,1\}^M$ are \emph{compatible} with the transmission vectors $t_{m\to c}, t_{f\to c}\in \{0,1\}^M$ if
\[h^0_c(k)=
 \begin{cases}
  h^0_m(k) & \text{if}\quad t_{m\to c}(k)=0 \\
  h^1_m(k) & \text{if}\quad t_{m\to c}(k)=1 \\
 \end{cases}
\]
and
\[h^1_c(k)=
 \begin{cases}
  h^0_f(k) & \text{if}\quad t_{f\to c}(k)=0 \\
  h^1_f(k) & \text{if}\quad t_{f\to c}(k)=1 \\
 \end{cases}
\]
for all $k\in\{1,\ldots,M\}$.
\end{definition}
With this notion of transmission vectors, recombination events are characterized by changes in the transmission vector, that is, by positions $k$ with $t_{m\to c}(k-1)\neq t_{m\to c}(k)$ or $t_{f\to c}(k-1)\neq t_{f\to c}(k)$.
Given our recombination cost vector $\mathcal{X}$, the cost associated to a transmission vector can be written as follows (in slight abuse of notation).

\begin{definition}[Transmission cost]
For a transmission vector $t_{p\to c}\in\{0,1\}^M$ with $p\in\{m,f\}$ and a recombination cost vector $\mathcal{X}\in\N^M$, the \emph{cost} of $t_{p\to c}$ is defined as
\[
  \mathcal{X}(t_{p\to c}) := \sum_{k=2}^M\iverl t_{p\to c}(k-1)\neq t_{p\to c}(k)\iverr\cdot\mathcal{X}(k),
\]
where $\iverl S \iverr=1$ if statement $S$ is true and $0$ otherwise.
\end{definition}

To state the problem of jointly phasing all individuals in $\mathcal{I}$ formally, it is instrumental to consider the set of matrix entries to be flipped explicitly.
We will therefore introduce a set of index pairs $E_i \subset \{1,\ldots,R_i\}\times\{1,\ldots,M\}$ where $(j,k)\in E_i$ if and only if the bit in row $j$ and column $k$ of matrix $\mathcal{F}_i$ is to be flipped.
\begin{figure}[t!]\centering
\includegraphics[width=\columnwidth]{{pedmec4}.pdf}
\caption{Example shows the input instance and cheapest solution and the resultant haplotypes.}
\label{fig:pedmec4}
\end{figure}
\begin{problem}[Weighted Minimum Error Correction on Pedigrees, PedMEC]\label{prob:pedmec}
Let a set of individuals $\mathcal{I}=\{1,\ldots,N\}$, a set of relationships $\mathcal{T}$ on $\mathcal{I}$, recombination costs $\mathcal{X}\in\N^M$, and, for each individual $i\in I$, a sequencing read matrix $\mathcal{F}_i\in\{0,1,-\}^{R_i\times M}$ and corresponding weights $\mathcal{W}_i\in\N^{R_i\times M}$ be given.
Determine a set of matrix entries to be flipped $E_i \subset \{1,\ldots,R_i\}\times\{1,\ldots,M\}$ to make $\mathcal{F}_i$ feasible and two corresponding haplotypes $h_i^0, h_i^1\in\{0,1\}^M$ for each individual $i\in\mathcal{I}$ as well as two transmission vectors $t_{m\to c},t_{f\to c}\in\{0,1\}^M$ for each trio $(m,f,c)\in\mathcal{T}$ such that
\[\sum_{i\in\mathcal{I}}\sum_{(j,k)\in E_i}\mathcal{W}_i(j,k)+\sum_{(m,f,c)\in\mathcal{T}}\mathcal{X}(t_{m\to c}) + \mathcal{X}(t_{f\to c})\]
takes a minimum, subject to the constraints that all haplotypes are compatible with the corresponding transmission vectors, if existing.
\end{problem}

Note that for the special case of $\mathcal{I}=\{1\}$ and $\mathcal{T}=\emptyset$, PedMEC is identical to wMEC.
Therefore, the PedMEC problem is also NP-hard.
As discussed in Section~\ref{sec:intro}, we are specifically interested in an application scenario were the genotypes are already known.
By using genotype data, we aim to most beneficially combine the merits of genetic haplotyping and read-based haplotyping.
We therefore extend the PedMEC problem to incorporate genotypes and term the resulting problem PedMEC-G.
\begin{problem}[PedMEC with genotypes, PedMEC-G]
Let the same input be given as for Problem~\ref{prob:pedmec} (PedMEC) and, additionally, a genotype vector $g_i\in\{0,1,2\}^M$ for each individual $i\in\mathcal{I}$. Solve the PedMEC problem under the additional constraints that $h^0_i+h^1_i=g_i$ for all $i\in\mathcal{I}$, where ``$+$'' refers to a component-wise addition of vectors.
\end{problem}

For the classical MEC problem, additionally assuming that all sites to be phased are heterozygous is common \citep{CDW13_exact}.
This variant of the MEC problem is a special case of PedMEC-G with $\mathcal{I}=\{1\}$ and $\mathcal{T}=\emptyset$ and $g_1(k)=1$ for all $k$.
\begin{figure}[t!]\centering
\includegraphics[width=\columnwidth]{{pedmeccost1}.pdf}
\caption{Example shows bipartition cost for a particular transmission 00 at column one.}
\label{fig:pedmeccost1}
\end{figure}
\section{Example on PedMEC}
Let us take an example to illustrate on how to solve the PedMEC instance shown in Figure~\ref{fig:pedmec4}.
Consider a trio with three individuals, mother, father and child. As input, the sequencing reads of each individual are represented as SNP matrices $\mathcal{F}_i$
Also, given corresponding weight matrices $\mathcal{W}_i$ that represent the likelihood of sequencing errors in each entry of matrices.
Also, we have a recombination vector $\mathcal{X}$, representing the likelihood of recombination between two consecutive variants.
The sequencing errors present in reads create conflicts in these matrices. 
The objective here is to generate the conflict-free matrices such that we obey Mendelian laws of inheritance.
This objective is achieved by jointly minimizing the flipping cost of set of entries over all the individuals and, additionally 
allowing minimum recombination events in mother, father or both, for every trio relationships.
As an output, we obtain two haplotypes for each individual in a trio.

In this small example, it is easy to compute that the entries marked in red boxes create conflict in matrices. 
Therefore, the cheapest solution is to flip these bits by paying a cost of 3 and 5, plus allowing one recombination event in mother for a cost of 22.
In this way, when we get conflict matrices for an each individual, we can assemble an each individual separately to generate two haplotypes for them.

Let us see on how we can solve this PedMEC instance using a dynamic programming based approach. 
Let us assume there are two haplotypes marked in green and purple for mother, and brown and blue for father shown in Figure~\ref{fig:pedmeccost1}.
The haplotypes for child can be determined based on the haplotypes transmitted from mother and father.
% We now explain the algorithm to solve PedMEC instances to generate two haplotypes for each individual.

In a DP, we go column wise from left and right. 
Let us consider first column and compute DP cell value for partitions shown in Figure~\ref{fig:pedmeccost1} and transmission value 00; 
the least significant bit 0 represents the haplotype green is transmitted to child, otherwise purple; and the most significant bit 0 tells that the
brown haplotype from father is transmitted to child, otherwise blue. 
To compute the minimum allele assignment for each partition, we try different possibilities of alleles assignments.
For example, first, we flip all the entries in the partition to 1; and pay cost based on weights for the corresponding entries from weight matrices and, second we try to flip entries to 0 and pay cost accordingly.
For the green partition, we pay cost 0 if we flip all the entries in the partition to 0 and otherwise 15+7+32+27 = 81.
Similarly, we compute the allele assignment costs for other partitions too.
We further take the minimum allele assignment for each partition and add the costs from all partitions, which results in cost 5 for this example.
Let us consider a different transmission value 01. For this transmission value, the haplotype purple is transmitted to child.
Accordingly, the partitions change and their corresponding allele assignment costs. For the green partition, which now does not span across individuals, cost 0 if we flip bits to 0 and otherwise cost 22.
As before, we compute the minimum allele assignment cost for each partition and resultant cost is 37 for this example.
% \begin{figure}[t!]\centering
% \includegraphics[width=\columnwidth]{{pedmeccost2}.pdf}
% \caption{Example shows biparition cost for a particular transmission 01 at column one.}
% \label{fig:pedmeccost2}
% \end{figure}
\begin{figure}[t!]\centering
\includegraphics[width=\columnwidth]{{pedmeccost4}.pdf}
\caption{Example shows bipartition cost for a particular transmission 00 at column two, given DP column for column one.}
\label{fig:pedmeccost4}
\end{figure}
Similarly, we can compute partitions cost for all possible partitions by considering different transmission value and store them in DP column.

Let us compute the partition cost for column two shown in Figure~\ref{fig:pedmeccost4}, given the DP column for column one.
In Figure~\ref{fig:pedmeccost4}, shown are partitions with transmission value 00, we compute the initialization cost as we computed before in column one. 
For example, for green partition, we compute the initialization cost (=3) as before, but we now additionally consider different possibilities of recombination events between two consecutive columns.
Therefore, we pay additional cost 91 if the DP cell at column one has transmission values 01 or 10, by allowing one recombination event for both cases. We do not pay any recombination cost without any recombination event.
In this way, when we compute the DP cell cost, we try all possibilities of recombination events (00, 01, 10, 11) with the previous cells and then take the minimum cost. 
Additionally, we consider the recursion cost from the column one such that the partitions are consistent.
For this example partition, it is easy to see that the total cost is 8.
Similarly, we compute partition cost for other partitions by trying all transmissions values and store them in DP column two.

We recurse this process till the last column. Once we know the optimal partitions at last column, we can finally backtrace to get the haplotypes for each individual.

With this example, we are now ready to provide a formal dynamic programming algorithm.

\section{Algorithm}\label{sec:algorithm}
\paragraph{Algorithm Overview: Solving PedMEC and PedMEC-G.}
In the following, we will see how the ideas of WhatsHap algorithm can be extended for solving PedMEC and PedMEC-G.
The basic idea is to use the same technique on the union of the sets of active reads across all individuals $i\in\mathcal{I}$, while adding some extra book-keeping to satisfy the additional constraints imposed by pedigree and genotypes.
Let $A_i(k)$ be the set of active reads in column $k$ of $\mathcal{F}_i$.
We now define $A(k)=\bigcup_{i\in\mathcal{I}}A_i(k)$.
% For convenience, we write $i(r)$ to denote the individual where read $r\in A(k)$ originated from, i.e.\ $r\in A_{i(r)}(k)$ for all $r\in A(k)$.
A bipartition $B=(P,Q)$ of $A(k)$ now induces bipartitions for each individual: $B_i=\big(P\cap A_i(k), Q\cap A_i(k)\big)$.

As before, we consider all bipartitions of $A(k)$ for each column $k$, but now additionally distinguish between all possible transmission values.
We assume the set of trio relationships $\mathcal{T}$ to be (arbitrarily) ordered and use a tuple $t\in\{0,1\}^{2|\mathcal{T}|}$ to specify an assignment of transmission values.
Such an assignment $t$ can later (during backtracing) be translated into the sought transmission vectors:
Assuming $t$ to be an optimal such tuple at column $k$, its relation to the transmission vectors is given by
\[t=\big(t_{m_1\to c_1}(k),t_{f_1\to c_1}(k),t_{m_2\to c_2}(k),t_{f_2\to c_2}(k),\ldots\big).\]

The transmission tuples give rise to one additional dimension of our DP table for PedMEC(-G), as compared to the DP table for wMEC.
For each column $k$, we compute table entries $C(k,B,t)$ for all $2^\abs{A(k)}$ bipartitions of reads and all $2^{2|\mathcal{T}|}$ possible transmission tuples, for a total of $2^{\abs{A(k)}+2\abs{\mathcal{T}}}$ entries in this column.

\paragraph{Computing Local Costs.}
Along the lines of \cite{Patterson2015}, we first describe how to compute the cost incurred by flipping matrix entries in each column, denoted by $\Delta_C(k,B,t)$, and then explain how to combine them with entries in $C(k-1,\cdot,\cdot)$ to compute the cost $C(k,B,t)$.
The crucial point for dealing with reads from multiple individuals in a pedigree is to realize that matrix entries from haplotypes that are identical by descent (IBD) need to be identical (or need to be flipped to achieve this).
For unrelated individuals (i.e. $\mathcal{T}=\emptyset$), none of the haplotypes are IBD, giving rise to $2|\mathcal{I}|$ sets of reads for the $2|\mathcal{I}|$ unrelated haplotypes.
These $2|\mathcal{I}|$ sets of reads are given by $B$ and the cost $\Delta_C(k,B,t)$ can be computed by flipping all matrix entries of reads within the same set to the same value.

For a non-empty $\mathcal{T}$, the transmission tuple $t$ tells which parent haplotypes are passed on to which child.
In other words, $t$ identifies each child haplotype to be IBD to a specific parent haplotype.
We can therefore merge the corresponding sets of reads since all reads coming from haplotypes that are IBD need to show the same allele and need to be flipped accordingly.
In total, we obtain $2|\mathcal{I}| - 2|\mathcal{T}|$ sets of reads, since each trio relationship implies merging two pairs of sets.
We write $\mathcal{S}(k,B,t)$ to denote this set of sets of reads induced by bipartition $B$ and transmission tuple $t$ in column $k$.
The cost $W_{k,S}^a$ of flipping all entries in a read set $S\in \mathcal{S}(k,B,t)$ to the same allele $a\in\{0,1\}$ is given by 
\[W_{k,S}^a = \sum_{(i,j)\in S}\iverl \mathcal{F}_i(j,k)\neq a\iverr\cdot\mathcal{W}_i(j,k),\]
where we identify reads in $S$ by a tuple $(i,j)$, telling that it came from individual $i$ and corresponds to row $j$ in $\mathcal{F}_i$.
For PedMEC, i.e.\ if no constraints on genotypes are present, every set $S$ can potentially be flipped to any allele $a\in\{0,1\}$.
Hence, the cost is given by 
\begin{equation}\label{eqn:delta_c}
\Delta_C(k,B,t)= \min_{a\in \{0,1\}^{\mathcal{S}(k,B,t)}}\left\{\sum_{S\in\mathcal{S}(k,B,t)}W_{k,S}^{a(S)}\right\},
\end{equation}
that is, we minimize the sum of costs incurred by each set of reads $S\in\mathcal{S}(k,B,t)$ over all possible assignments of alleles to read sets.
For PedMEC-G, this minimization is constrained to only consider allele assignments consistent with the given genotypes.
To ensure that valid assignments exist, we assume the input genotypes to be free of Mendelian conflicts.

\paragraph{Computing a Column of Local Costs.}
To compute the whole column $\Delta_C(k,\cdot,\cdot)$, we proceed as follows.
In an outer loop, we enumerate all $2^{2\abs{\mathcal{T}}}$ values of the transmission tuple $t$.
For each value of $t$, we perform the following steps:
We start with bi-partition $B=(A(k),\emptyset)$ and compute all $W_{k,S}^a$ for all sets $S\in\mathcal{S}(k,B,t)$ and all $a\in\{0,1\}$, which can be done in $\mathcal{O}(\abs{A(k)}+|\mathcal{I}|)$ time.
Next we enumerate all bipartitions in Gray code order, as done previously \citep{Patterson2015}.
This ensures that only one read is moved from one set to another in each step, facilitating constant time updates of the values $W_{k,S}^a$.
The value of $\Delta_C(k,B,t)$ is then computed from the $W_{k,S}^a$'s according to Equation~\eqref{eqn:delta_c}, which takes $\mathcal{O}(2^{2|\mathcal{I}|}\cdot |\mathcal{I}|)$ time.
Computing the whole column $\Delta_C(k,\cdot,\cdot)$ hence takes 
$\mathcal{O}\big(2^{2\abs{\mathcal{T}}}\cdot(2^{|A(k)|} +  2^{2|\mathcal{I}|}\cdot |\mathcal{I}|)\big)$
time.

\paragraph{DP Initialization.}
The first column of the DP table, $C(1,\cdot,\cdot)$, is initialized by setting $C(1,B,t):=\Delta_C(1,B,t)$ for all bipartitions $B$ and all transmission tuples $t$.

\paragraph{DP Recurrence.}
Recall that $C(k,B,t)$ is the cost of an optimal solution for input matrices restricted to the first $k$ columns under the constraints that the sought bipartition extends $B$ and that transmission happened according to $t$ at site $k$.
Entries in column $C(k+1,\cdot,\cdot)$ should hence add up local costs incurred in column $k+1$ and costs from the previous column. 
To adhere to the semantics of $C(k+1,B,t)$, only entries in column $k$ whose bipartitions are \emph{compatible} with $B$ are to be considered as possible ``predecessors'' of $C(k+1,B,t)$.
\begin{definition}[Bipartition compatibility]
Let $B=(P,Q)$ be a bipartition of $A$ and $B'=(P',Q')$ be a bipartition of $A'$.
We say that $B$ and $B'$ are \emph{compatible}, written $B\simeq B'$, if
$P\cap(A\cap A') = P'\cap(A\cap A')$
and
$Q\cap(A\cap A') = Q'\cap(A\cap A')$.
\end{definition}
Two bipartitions are therefore compatible when they agree on the intersection of the underlying sets.
Besides ensuring that bipartitions are compatible, we need to incur recombination costs in case the transmission tuple $t$ changes from $k$ to $k+1$.
Formally, entries in column $k+1$ are given by
\begin{align}\label{eqn:recurrence}
& C(k+1,B,t)= \Delta_C(k+1,B,t)\\
& \quad + \min_{\substack{B'\in\mathcal{B}(A(k)):B'\simeq B\\t'\in\{0,1\}^{2\abs{\mathcal{T}}}}}\big\{C(k,B',t')+d_H(t,t')\cdot\mathcal{X}(k+1)\big\}, \nonumber
\end{align}
where $\mathcal{B}\big(A(k)\big)$ denotes the set of all bipartitions of $A(k)$ and $d_H$ is the Hamming distance.
The distance $d_H(t,t')$ hence gives the number of changes in transmission vectors and thus the term $d_H(t,t')\cdot\mathcal{X}(k+1)$ gives the recombination cost to be added.

\paragraph{Projection Columns.}
To ease computing $C(k+1,B,t)$ via Equation~\eqref{eqn:recurrence}, we use the same technique described by \cite{Patterson2015} and define intermediate \emph{projection columns} $C^\cap(k,\cdot,\cdot)$.
They can be thought of as being \emph{between} columns $k$ and $k+1$.
Consequently, they are concerned with bipartitions of the intersection of read sets $A(k)\cap A(k+1)$ and hence contain $2^{|A(k)\cap A(k+1)|+2|\mathcal{T}|}$ entries, which are given by
\begin{equation}\label{eqn:proj_col}
C^\cap(k,B',t)=\min_{\mathcal{B}(A(k)):B\simeq B'}\{C(k,B,t)\}.
\end{equation}
These projection columns can be created while computing $C(k,\cdot,\cdot)$ at no extra (asymptotic) run-time.
Using these projection columns, Equation~\eqref{eqn:recurrence} becomes
\begin{align}\label{eqn:recurrence_projcol}
& C(k+1,B,t)= \Delta_C(k+1,B,t)\\
& \quad + \min_{t'\in\{0,1\}^{2\abs{\mathcal{T}}}}\big\{C^\cap(k,B\cap A(k),t')+d_H(t,t')\cdot\mathcal{X}(k+1)\big\}, \nonumber
\end{align}
where $B\cap A(k) := (P\cap A(k), Q\cap A(k))$ for $B=(P,Q)$.
We have therefore reduced the run-time of computing this minimum to $\mathcal{O}(2^{2|\mathcal{T}|})$.

\paragraph{Runtime.}
Computing one column of local costs, $\Delta_C(k,\cdot,\cdot)$, takes $\mathcal{O}\big(2^{2\abs{\mathcal{T}}}\cdot(2^{|A(k)|} +  2^{2|\mathcal{I}|}\cdot |\mathcal{I}|)\big)$ time, as discussed above.
For each entry, we use Equation~\eqref{eqn:recurrence_projcol} to compute the aggregate value of cost incurred in present and past columns.
Over all columns, we achieve a run-time of 
$\mathcal{O}\big(M\cdot \big(2^{2|\mathcal{T}|+c+|\mathcal{I}|}|\mathcal{I}|+2^{4|\mathcal{T}|+c}\big)\big)$, where $c=\max_k\{\abs{A(k)}\}$ is the maximum coverage.

\paragraph{Backtracing.}
An optimal bipartition and transmission vectors can be obtained by recording the indexes of the table entries that gave rise to the minima in equations \eqref{eqn:recurrence_projcol} and \eqref{eqn:proj_col} when filling the DP table and then backtracing starting from the optimal value in the last column.
Optimal haplotypes are subsequently obtained using the bipartition and transmission vectors.


\section{Experimental Setup}
To evaluate the performance of our approach, we considered both real and simulated datasets.

\subsection{Real Data}
The Genome in a Bottle Consortium (GIAB) has characterized seven individuals extensively using eleven different technologies~\citep{giab}.
The data is publicly available.
Here we consider the Ashkenazim trio, consisting of three related individuals: NA24143 (mother), NA24149 (father) and NA24385 (son).
We obtained a consensus genotype call set (NIST\_CallsIn2Technologies\_05182015) provided by GIAB, containing variants called by two independent technologies.
For our benchmark, we consider all bi-allelic SNPs on Chromosome~1 called in all three individuals, amounting to 141,256 in total, and use the provided (unphased) genotypes.

\paragraph{Ground Truth via Statistical Phasing.}
To generate a ground truth phasing for comparison, we used the population-based phasing tool SHAPEITv2-r837~\citep{shapeit} with default parameters.
The program was given the 1000 Genomes reference panel\footnote{\scriptsize{\url{https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3.tgz}}}, the corresponding genetic map\footnote{\scriptsize{\url{http://www.shapeit.fr/files/genetic_map_b37.tar.gz}}}, and the unphased genotypes as input.
SNPs present in the GIAB call set but absent in the reference panel were discarded, resulting in 140,744 phased SNPs, of which 58,551 were heterozygous in mother, 57,152 in father and 48,023 in child.
We refer to this set of phased SNPs as \emph{ground truth phased variants}.
We emphasize that this phasing is solely based on genotypes and does not use phase information present in the reads in any way and hence is completely independent.
In the following, we refer to the original genotypes from the GIAB call set (without phase information) restricted to this set of SNPs as \emph{ground truth unphased genotypes}, which we use as input for read-based phasing experiments described below.

\paragraph{PacBio Data.} For each individual, we downloaded aligned Pacific Biosciences (PacBio) reads\footnote{\scriptsize{\url{ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/(HG002_NA24385_son|HG003_NA24149_father|HG004_NA24143_mother)/PacBio_MtSinai_NIST/MtSinai_blasr_bam_GRCh37/}}},  
which had an average coverage of 42.3$\times$ in mother, 46.8$\times$ in father and 60.2$\times$ in child, respectively.
The average mapped read length across mother was 8,328 bp, father was 8,471 bp and child was 8,687 bp.
For each individual, we separately downsampled the aligned reads to obtain data sets of 2$\times$, 3$\times$, 4$\times$, 5$\times$, 10$\times$, and 15$\times$ average coverage.

\paragraph{10XGenomics Data.}
The GemCode platform marketed by 10XGenomics uses a barcoding technique followed by pooled short-read sequencing and data analysis through a proprietary software solution to resolve haplotypes.
Data from this platform is available from the GIAB project and represents phase information obtained completely independently from either statistical phasing or PacBio reads.
We downloaded the corresponding files\footnote{\scriptsize{\url{ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/(HG002_NA24385_son|HG003_NA24149_father|HG004_NA24143_mother)/10XGenomics}}} for comparison purposes.

\subsection{Simulated Data}
Despite the high-quality data set provided by GIAB, we sought to complement our experiments by a simulated data set.
While the population-based phasing we use as ground truth is arguably accurate due to a large reference panel and the high-quality genotype data used as input, it is not perfect.
Especially variants with low allele frequency present challenges for population-based phasers.

\paragraph{Virtual Child.}
As basis for our simulation, we use the haplotypes of the two parents from our ground truth phased dataset.
We generated two haplotypes of a virtual child by applying recombination and Mendelian inheritance to the four parent haplotypes.
In reality, recombination events are rare: All of Chromosome~1 spans a genetic distance of approximately 292\,cM, corresponding to 2.9 expected recombination events along the whole chromosome.
To include more recombinations in our simulated data set, we used the same genetic map as above, but multiplied recombination rates by 10.
The recombination sites are sampled according to the probabilities resulting from applying Haldane's mapping function to the genetic distances between two variants.
In line with our expectation, we obtained 26 and 29 recombination sites for mother and father, respectively. 
The resulting child had 41,676 heterozygous variants.

\paragraph{Simulating PacBio Reads.}
We aimed to simulate reads that mimic the characteristics of the real PacBio data set as closely as possible.
For this simulation, we incorporate the variants of each individual into the reference genome (hg19) to generate two true haplotypes for each individual in our triple.
We used the PacBio-specific read simulator pbsim by \cite{pbsim} to generate a 30$\times$ data set for Chromosome~1.
The original GIAB reads were provided to pgsim as a template (via option \texttt{--sample-fastq}) to generate artificial reads with the same length profile.
Next, we aligned the reads to the reference genome using BWA-MEM 0.7.12-r1039 by \cite{bwa} with option \texttt{-x pacbio}.
As before for the real data, the aligned reads for each individual were downsampled separately to obtain data sets of 2$\times$, 3$\times$, 4$\times$, 5$\times$, 10$\times$, and 15$\times$ average coverage.

\subsection{Compared Methods}
Our main goal is to analyze the merits of the PedMEC-G model in comparison to wMEC; in particular with respect to the coverage needed to generate a high-quality phasing.
The algorithms to solve wMEC and PedMEC-G described in Section~\ref{sec:algorithm} have been implemented in the WhatsHap software package\footnote{\scriptsize{\url{https://bitbucket.org/whatshap/whatshap}}}, distributed as Open Source software under the terms of the MIT license.
We emphasize that WhatsHap solves wMEC and PedMEC-G optimally.
Since the focus of this paper is on comparing these two models, we do not include other methods for single-individual haplotyping.
We are not aware of other trio-aware read-based phasing approaches that PedMEC-G could be compared to additionally.

The run-time depends exponentially on the maximum coverage.
Therefore we prune the input data sets to a \emph{target maximum coverage} using the read-selection method introduced by \cite{Fischer2016}, which is implemented as part of WhatsHap.
This target coverage constitutes the only parameter of our method.
For PedMEC-G, we prune the maximum coverage to 5$\times$ for each individual separately.
For wMEC, we report results for 5$\times$ and 15$\times$ target coverage.
The respective experiments are referred to as \mbox{PedMEC-G-5}, wMEC-5, and wMEC-15.
For wMEC, we use the additional ``all heterozygous'' assumption (see Section~\ref{sec:algorithm}), to also give it the advantage of being able to ``trust'' the genotypes, as is the case for PedMEC-G.
Both wMEC and PedMEC-G were provided with the ground truth unphased genotypes for the respective data set.
PedMEC-G was additionally provided with the respective genetic map (original 1000G genetic map for real data and scaled by factor 10 for simulated data).
\begin{figure}%
\centering
\includegraphics[width=\columnwidth,height=.7\columnwidth,keepaspectratio]{{simulated}.pdf}\\
\centering
\includegraphics[width=\columnwidth,height=.7\columnwidth,keepaspectratio]{{pedmec1}.pdf}%
\caption{Simulated data set (top) and real dataset (bottom): phasing error rate ($x$-axis) versus completeness in terms of the fraction of unphased SNPs ($y$-axis) for \mbox{PedMEC-G-5} (solid line), wMEC-5 (dashed line), and wMEC-15 (dotted line).
Average coverage (per individual) of input data is encoded by circles of different sizes.}\label{fig:main-results}%
\end{figure}

As described above, the ground truth phased variants was generated by SHAPEIT with default parameters, implying that SHAPEIT treated the three samples as unrelated individuals.
For comparison purposes, we re-ran SHAPEIT and provided it with pedigree information.
We refer to the resulting phased data set as \emph{SHAPEIT-trio}.
Moreover, we ran duoHMM (v0.1.7) by \cite{OConnell2014} on the resulting files to further improve the phasing.
\begin{figure}[t]
\begin{center}
\includegraphics[height = .6\columnwidth, width=.6\columnwidth]{{Marschall.152.fig.3}.pdf}
\end{center}
\caption{Three-way comparison of phasings provided by SHAPEIT, 10XGenomics, and \mbox{PedMEC-G-5} (on 15$\times$ coverage data). 
Of all pairs of consecutive SNPs phased by all three methods, the percentages of cases where the phasing reported by one method disagrees with the other two are reported. 
Missing to 100\%: cases where all three methods agree.
Left: SHAPEIT run with default parameters, corresponding to our ``ground truth phasing''; right: SHAPEIT run with pedigree information.}\label{fig:threeway}
\end{figure}
\section{Performance Metrics}\label{sec:metrics}
We compare each phased individual to the respective ground truth haplotypes separately and only consider sites heterozygous in this individual.

\paragraph{Phased SNPs.}
For read-based phasing of a single individual (wMEC), we say that two heterozygous SNPs are directly connected if there exists a read covering both.
We compute the connected components in the graph where SNPs are nodes and edges are drawn between directly connected SNPs.
Each connected component is called a \emph{block}.
For read-based phasing of a trio (PedMEC), we draw an edge when two SNPs are connected by a read in any of the three individuals.
In both cases, we count a SNP as being \emph{phased} when it is not the left-most SNP in its block (for the left-most SNP, no phase information with respect to its predecessors exists).
All other SNPs are counted as \emph{unphased}.
Below, we report the average \emph{fraction of unphased SNPs} over all three family members.

\paragraph{Phasing Error Rate.}
For each block, the first predicted haplotype is expressed as a mosaic of the two true haplotypes, minimizing the number of switches.
This minimum is known as the \emph{number of switch errors}.
Note that the second predicted haplotype is exactly the complement of the first one, due to only considering heterozygous sites.
When two switch errors are adjacent, they are subtracted from the number of switch errors and counted as one \emph{flip error}.
The \emph{phasing error rate} is defined as the sum of switch and flip errors divided by the number of phased SNPs.

\paragraph{Three-Way Phasing Comparison.}
To simultaneously compare phasings from three different methods (e.g.\ SHAPEIT, 10XGenomics, and \mbox{PedMEC-G-5}), we proceeded as follows.
For an individual, we considered all pairs of consecutive heterozygous SNPs that have been phased by all three methods.
For each of these pairs, either all three methods agree or two methods agree (since only two possible phases exist).
Below, we discuss the fraction of these different cases in relation to the total number of considered SNP pairs.
\begin{figure}[t]\centering
\includegraphics[width=\columnwidth]{{Marschall.152.fig.4}.pdf}
\caption{Two disjoint unconnected haplotype blocks for which phase information can be inferred from the genotypes.}
\label{fig:genetic_molecular}
\end{figure}
\section{Results}
We report the results of  wMEC-5, wMEC-15, and \mbox{PedMEC-G-5} for both data sets, \emph{real} and \emph{simulated}.
All combinations of the three methods, two data sets, and six different average coverages (2$\times$, 3$\times$, 4$\times$, 5$\times$, 10$\times$ and 15$\times$) were run.
The predicted phasings are compared to the ground truth phasing for the respective data set.
That is, for the real data set, we compare to the population-based phasing produced by SHAPEIT; for the simulated data set, we compare to the true haplotypes that gave rise to the simulated reads.
Figure~\ref{fig:main-results} shows the fraction of unphased SNPs in comparison to the phasing error rate (see Section~\ref{sec:metrics}) for all conducted experiments.
A perfect phasing would be located in the bottom left corner.

\paragraph{The Influence of Coverage.}
Increasing the average coverage is beneficial for phasing.
For all three methods (wMEC-5, wMEC-15, and \mbox{PedMEC-G-5}) and both data sets, the phasing error rate and the fraction of unphased SNPs decrease monotonically when the average coverage is increased, as is clearly visible in Figure~\ref{fig:main-results}.
The effect is much more drastic for wMEC than for PedMEC, however.
Apparently, wMEC needs more coverage to compensate for PacBio's high error rate while PedMEC can resort to exploiting family information to resolve uncertainty.

\paragraph{The Value of Family Information.}
When operating on the same input coverage, \mbox{PedMEC-G-5} clearly outperformed wMEC-5 and even wMEC-15 in all cases tested.
This was true for phasing error rates as well as for the fraction of phased positions.
On the real data set with average coverage 10$\times$, for instance, wMEC-5 and wMEC-15 reached an error rate of 2.9\% and 1.9\%, while it was 0.5\% for \mbox{PedMEC-G-5}.

Most remarkingly, \mbox{PedMEC-G-5} delivers excellent results already for very low coverages.
When working with an average coverage as low as 2$\times$ for each family member, it achieves an error rate of 1.4\% and a fraction of unphased SNPs of 1.8\% (on real data).
In contrast, wMEC-15 needs 15$\times$ average coverage on each individual to reach similar values (1.4\% error rate and 1.3\% unphased SNPs).
When running on 5$\times$ data, \mbox{PedMEC-G-5}'s error rate and fraction of unphased positions decrease to 0.75\% and 0.85\%, respectively.
Therefore, it reaches better results while requiring only a third of the sequencing data, which translates into significantly reduced sequencing costs.

\paragraph{Comparison of Real and Simulated Data.}
When comparing results for simulated and real data, i.e.\ top and bottom plots in Figure~\ref{fig:main-results}, the curves appear similar, with some important difference.
In terms of the fraction of phased SNPs ($y$-axis), results are virtually identical.
This indicates that our simulation pipeline establishes realistic conditions regarding this aspect.
Differences in terms of error rates ($x$-axis) are larger.
In general, error rates in the real data are larger than in the simulated data, which might be partly caused by a too optimistic error model during read simulation.
On the other hand, the population-based phasing used as ground truth for the real data set will most likely also contain errors.
Especially low-frequency variants present difficulties for population-based phasers.
Next, we therefore compare our ground truth statistical phasing to the independent phasing provided by 10XGenomics.

\paragraph{Three-Way Comparison with 10XGenomics.}
Figure~\ref{fig:threeway}(left) shows the results of a three-way comparison of ground truth statistical phasing, 10XGenomics, and \mbox{PedMEC-G-5} on 15$\times$ coverage data.
We observe that the total fraction of cases where there is disagreement between the three methods is below 1\%.
Out of these, 10XGenomics and the statistical phasing agree in a sizeable fraction of cases, shown in blue.
In these cases, the PacBio-based \mbox{PedMEC-G-5} is likely wrong.
Given PacBio's high read error rate, the existence of such cases is not surprising.
On the other hand, there also is a significant fraction of cases where \mbox{PedMEC-G-5} and 10XGenomics agree, shown in red, indicating likely errors in the statistical phasing.
Cases where \mbox{PedMEC-G-5} and the statistical phasing agree but disagree with 10XGenomics are very rare, which is likely due to the low error rates of short-read sequencing underlying the 10XGenomics phasing and the resulting highly accurate phasing.

\paragraph{SHAPEIT-trio and duoHMM.}
Figure~\ref{fig:threeway}(right) shows the same three-way comparison, but uses the results obtained from SHAPEIT when run in trio mode.
We see that this improved the phasing for the child but dramatically worsened the agreement for the parents, with more than 4\% of all phased SNP pairs for which 10XGenomics and \mbox{PedMEC-G-5} agreed but disagreed with SHAPEIT-trio.
Running duoHMM \citep{OConnell2014} to improve the SHAPEIT-trio phasing did not lead to any changes, which might be related to that duoHMM is designed to be run for large cohorts of related individuals.

\paragraph{Phase Information Beyond Block Boundaries.}
Genetic phasing operates on genotypes of a pedigree, without using any sequencing reads.
Figure~\ref{fig:genetic_molecular} illustrates a case where we have two blocks that are not connected by reads in any individual. 
Nonetheless phase information can be inferred from the genotypes: Each block contains a SNP that is homozygous in both parents and heterozygous in the child, which immediately establishes which haplotype is maternal and which is paternal in both blocks.
Note that this, in turn, also implies the phasing of the parents.
By design, PedMEC-G implicitly exploits such information.
To demonstrate this, we used the real data set and merged all blocks reported by PedMEC-G into one chromosome-wide block and determined the fraction of cases where phases were correctly inferred between blocks---and hence between two SNPs that are not connected by reads in any individual.
This resulted in a fraction of 89.7\% correctly inferred phased (averaged over all individuals and coverages; standard deviation 1.4\%).
Repeating the same for wMEC yielded 50.4\% correctly inferred phased, as expected equalling a coin flip.

\paragraph{Runtimes.}
All experiments have been run on a server with two Intel Xeon E5-2670 CPUs (10 cores each) running at 2.5GHz.
The implementation in WhatsHap is sequential, i.e.\ only using one CPU core.
In all cases, the time spent reading the input files dominated the time spent in the phasing routine itself.
Processing all three individuals of the 5$\times$ coverage real data set took 31.1\,min, 31.2\,min, and 26.2\,min for wMEC-5, wMEC-15, and \mbox{PedMEC-G-5}, respectively.
This time included all I/O and further processing.
Of these times, 2.0\,s, 4.1\,s, and 101.0\,s were spent in the phasing routine, respectively.
For input coverage 15$\times$, total processing took 89.3\,min, 93.9\,min, and 65.4\,min for wMEC-5, wMEC-15, and \mbox{PedMEC-G-5}, respectively.
Of this, 2.5\,s, 149.9\,s, 321.5\,s were spent in the phasing routines, respectively.
We conclude that the phasing algorithm presented here is well suited for handling current data sets swiftly.
In the future, we plan to further optimize the implementation of I/O subroutines and provide automatic chromosome-wise parallelization of data processing.

\section{Discussion}
We have presented a unifying framework for integrated read-based and genetic haplotyping.
By generalizing the WhatsHap algorithm \citep{Patterson2015}, we provide a fixed-parameter tractable method for solving the resulting NP-hard optimization problem, which we call \emph{PedMEC}.
When maximum coverage and number of individuals are bounded, the algorithm's run-time is linear in the number of phased variants and independent of the read length, making it well suited for current and future long-read sequencing data.
This is mirrored by the fact that the run-time is dwarfed by the time required for reading the input files in practice.
PedMEC can use any provided costs for correcting errors in reads as well as for recombination events.
By using phred-scaled probabilities as costs, minimizing the cost can be interpreted as finding a maximum likelihood phasing in a statistical model incorporating Mendelian inheritance, read error correction, and recombination.

Testing the implementation on simulated and real trio data, we could show that the method is notably more accurate than phasing individuals separately, especially at low coverages.
Beyond enhanced accuracy, our method is also able to phase a greater fraction of heterozygous variants compared to single-individual phasing.

Being able to phase more variants is a key benefit of the integrative approach.
Whereas read-based phasing can in principle only phase variants connected by a path through the covering reads, adding pedigree information enables even phasing of variants that are not covered in all individuals since the algorithm can ``fall back'' to using genotype information.
Figure~\ref{fig:ex_pedigree} illustrates the increased connectivity while phasing a trio, resulting in more phased variants in practice.

Genetic haplotyping alone cannot phase variants that are heterozygous in all individuals, emphasizing the need for an integrative approach as introduced here.
We demonstrate that such an approach indeed yields better result and recommend its use whenever both reads and pedigree information are available.
Most remarkingly, the presented approach is able to deliver outstanding performance even for coverages as low as 2$\times$ per individual, on par with performance delivered by single-individual haplotyping at 15$\times$ coverage per individual.

The disadvantage of PedMec framework is that we can not phase variants that are heterozygous in all individuals.
Additionally, the variants that are unique to the genome can not be phased due to the reference genome being included in the pipeline.
The next step is to generalize this approach by excluding a reference genome from the pipeline and perform phasing directly from reads.
In the next chapter, we discuss the generalized approach to generate haplotypes directly from reads.

% \paragraph{Future work.}
% We plan to implement phasing of \emph{de novo} variants observed in the child, which would be impossible with pure genetic haplotyping but is straightforward with our approach.
% 
% Since runtime is exponential in the maximum physical coverage, pruning of datasets is required in practice.
% The read selection approach currently implemented in WhatsHap \citep{Fischer2016} aims to retain reads that both cover and connect many variants at the same time, in particular for heterogeneous combinations of datasets such as paired-end or mate-pair reads together with long reads.
% For pedigrees, each dataset is currently pruned individually, but results would likely improve if pedigree structure was taken into account in this step.
% Finally, since we show that it is possible and beneficial to integrate both read-based and genetic phasing, the next obvious question is whether it is possible to modify our unified theoretical framework to one that also includes statistical phasing.






%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
