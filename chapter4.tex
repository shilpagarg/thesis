\chapter{A graph-based approach to diploid assembly}
Constructing high-quality haplotype-resolved \textit{de novo} assemblies of diploid genomes is important to reveal the full extent of structural variation and its role in health and disease.
Current assembly approaches often collapse the two sequences into one haploid consensus sequence and, therefore, fail to capture the diploid nature of the organism under study.
Thus, building an assembler able to produce accurate and complete diploid assemblies, while being resource-efficient with respect to sequencing costs, is a key challenge to be addressed by the bioinformatics community.\\

In this chapter, we present a novel graph-based approach to diploid assembly, which combines accurate Illumina data and long-read Pacific Biosciences (PacBio) data.
We demonstrate the effectiveness of our method on a pseudo-yeast diploid genome and show that we require as little as 50$\times$ coverage Illumina data and 10$\times$ PacBio data to generate accurate and complete assemblies.
Additionally, we show that our approach has the ability to detect and phase structural variants.\\


\section{Introduction}
Determining the two genome sequences per chromosome of diploid organisms is important.
Separate determination of the two haplotype sequences can in principle avoid genotyping errors in complex regions of the genome caused by incorrect models of variants at nearby sites being independent.

As introduced in Chapter 1, the process of assembling the two genome sequences from sequencing reads in a haplotype-aware manner is known as \textit{diploid} or \textit{haplotype-aware genome assembly} and the generated assemblies are known as ``haplotigs''.
However, next generation sequencing (NGS) reads are generally of a variable, short length and contain errors; therefore, solving the diploid genome assembly problem is fundamentally challenging.
Additional challenges inherent in the genome assembly problem include dealing with short and long genomic repeats, handling other rearrangements present in the genome, and scaling efficiently with input size, genome size, and hardware availability.

Over the last decade, the development of various NGS technologies has impacted the assembly problem.
In theory, the problem of \textit{de novo assembly}---computing the consensus of two or more sequences---is NP-hard, when the problem is modelled either as string graphs or de Bruijn graphs \citep{medvedev2007computability}. 
There are several heuristic approaches to approximate the optimal de novo haploid assembly based on NGS datasets \citep{idury1995new, myers1995toward, myers2005fragment, pevzner2001eulerian, nagarajan2009parametric, nagarajan2013sequence, sovic2013approaches}.

However, even with Sanger (reads of the order of 800-1000 base pairs) and Illumina sequencing, which deliver short reads with low error rates, de novo assembly of heterozygous diploid genomes has been a difficult problem \citep{vinson2005assembly, levy2007diploid}.
In practice, there are several short-read assemblers based on Illumina data for heterozygous genomes \citep{kajitani2014efficient, pryszcz2016redundans, simpson2012efficient, bankevich2012spades, li2015fermikit}.
The assemblies that they produce are accurate, but contain gaps and are composed of relatively short contigs and scaffolds. 
Third generation sequencing technologies such as methods available from Pacific Biosciences (PacBio) and Oxford Nanopore Technologies (ONT) deliver much longer reads, but with high error rates.
There are now several long-read assemblers \citep{koren2017canu, vaser2017fast, xiao2016mecat, berlin2015assembling, chin2013nonhybrid, hunt2015circlator, lin2016assembly} that use these long-read data for de novo assembly.
The assemblies that are delivered from these assemblers are more contiguous, with longer contigs and scaffolds.
Finally, there are hybrid assemblers that take advantage of long-read data (with its high error rate) and short-read data (with its low error rate) \citep{bashir2012hybrid, antipov2015hybridspades, zimin2017hybrid} and attempt to combine the best aspects of both.
These hybrid assemblers have the power to deliver highly accurate, repeat-resolved assemblies.

However, across the short, long, and hybrid categories, most assemblers require collapsing the two genome sequences of a diploid sample into a single haploid ``consensus'' sequence (or primary contig). 
The consensus sequence is obtained by merging the distinct alleles at regions of heterozygosity into a single allele, and therefore losing a lot of information.
The resulting haploid de novo assembly does not represent the true characteristics of the diploid input genome. 

To generate diploid assemblies for heterozygous genomes, there are two standard linear approaches; one uses haploid contig sequences \citep{chin2016phased, pendleton2015assembly, seo2016novo, mostovoy2016hybrid}, while the other partitions the reads while using the reference genome as a backbone \citep{glusman2014whole, martin2016whatshap, chaisson2017multi}.
% \todo{Maybe we should cite the HGSVC preprint. Tobi: I can insert this.}
In both approaches, the reads are first aligned (either to the reference genome or the contigs). Second, variants such as SNVs are called based on the aligned reads. Finally, the detected variants are phased using long reads from a same or different sequencing technology.

For both reference-guided and contig-based assembly, this third step---solving the phasing problem---has been formulated as the minimum error correction (MEC) optimization problem \citep{Lippert:2002ba,cilibrasi2007complexity}.
There are several disadvantages to reference-guided assembly; for example, the reads are initially aligned to the reference genome and therefore the process contains a reference bias. This approach can fail to detect sequences or large structural variants that are unique to the genome being assembled.

However, there are also several reasons why the set of sequences/contigs produced by contig-based assembly is not ideal.
First, the contigs produced by assemblers ignore the heterozygous variants in complex regions, opting instead to break contiguity to express even moderate complexity. 
Second, the contigs do not capture end-to-end information in the genome; the ordering or relationships between contigs are critical in order to generate end-to-end chromosomal-length assemblies.
% \todo{TM: I do not understand this sentence. Can you explain / make this more clear?}
% \sgnote{Done.}

Some newer diploid assembly methods include \cite{weisenfeld2017direct}, where 10x Genomics linked read data is used to determine the actual diploid genome sequence. 
Their approach is based on de Bruijn graphs and applies a series of graph simplifications, where simple bubbles are detected and phased by using (short) reads that stem from the same (long) input molecule, which is determined through barcoding.
There is also a recent study by \cite{chin2016phased}, who follow a linear phasing approach to generate diploid assemblies (\textit{haplotigs}) for diploid genomes by using PacBio reads.

\paragraph{Contributions.}
We propose a graph-based approach to generate haplotype-aware assemblies of single individuals.
Our contribution is twofold. First, we propose a hybrid approach to integrate accurate Illumina and long PacBio reads to generate diploid assemblies. 
The Illumina reads are used to generate an assembly graph that serves as a backbone for subsequent PacBio-based steps.
Second, we generalize the diploid assembly problem to encompass constructing the diploid assembly directly from the underlying assembly graph.
The two genome sequences can be seen as two paths over the regions of heterozygosity in the assembly graph.
We make some first steps towards performing read-based phasing on graphs.

% Phasing using an assembly graph has several advantages over other approaches. For example, it is possible to phase larger blocks at once, because paths in an assembly graph can span multiple variants.
% Moreover, it is easier to detect large structural variants, such as translocations and other rearrangements, in an assembly graph.
% The structural variants are reflected by bubbles in an assembly graph. The bubbles are defined as a set of disjoint paths that share the same start and end nodes. 
% Figure~\ref{fig:ex_sv} illustrates how such bubbles can represent both small variants (SNVs and indels up to 50 base pairs in length) and larger structural variants.
% The graph-based approach provides a way to both accurately detect all types of structural variation and perform further downstream analyses.
% Figure~\ref{fig:ex_graph_approach} shows the conceptual advantage of our graph-based approach over contig-based methods such as Falcon Unzip.
% Consider four SNVs separated by two large SVs and there are four reads spanning these variants.
% Out of those reads, the two reads $r_3$ and $r_4$ span the two SVs, but do not cover any of the two SNVs.
% In this case, Falcon Unzip generates a primary contig that spans from one end to the other, but generates incomplete and fragmented haplotigs (phased primary contigs in the language of Falcon Unzip).
% % Since we are interested in haplotypes, we talk in the language of \textit{haplotigs} in the rest of the article.
% % \todo{Commented out a sentence that disrupted the flow. Better define ``haplotig'' already above?}.
% % \sgnote{I added it in the second para of Introduction.}
% In contrast, our graph-based approach attempts to phase across all types of variation, including SVs, in order to produce end-to-end haplotigs.
% Therefore, the graph-based approaches are powerful to deliver more complete and contiguous haplotigs.
% In Section~\ref{sec:phasing} we discuss in detail how we can phase these bubbles using graph-based approach. 

We demonstrate the feasibility of our approach by performing a haplotype-aware de novo assembly of a whole pseudo-diploid yeast (SK1+Y12) genome.
We show that we generate more accurate, more contiguous, and more correctly phased diploid genomes compared to Falcon Unzip.
Through analysis of different coverage levels, we demonstrate that we require only 50$\times$ short-read coverage and as little as 10$\times$ long-read coverage data to generate diploid assemblies.
This illustrates that our hybrid strategy is a cost-effective way of generating haplotype-resolved assemblies.
% Our hybrid assembler, which operates on both long reads (with low coverage) and short reads (with higher coverage), has the potential to generate high-quality assemblies at reduced cost.
Finally, we show that we successfully detect and phase large structural variants.


% \begin{figure}[t!]\centering
% \includegraphics[width=\columnwidth]{ex_sv.pdf}
% \caption{Based on reads (middle) from the two sequences (top), the bubbles in the graph (bottom) show three different heterozygous variations; the first one is an SNV, the second one is an SV, and the third one is an indel. }
% \label{fig:ex_sv}
% \end{figure}

% \begin{figure}[t!]\centering
% \includegraphics[width=\columnwidth]{ex_graph_approach.pdf}
% \caption{Input: an assembly graph (top) (consisting of four SNVs and two SVs) and the PacBio reads $r_1, r_2, r_3, r_4, r_5, r_6$ (gray). Output: the phased reads (colored in blue and red) and haplotigs (bottom) using Falcon Unzip and our graph-based approach. 
% Our graph-based approach delivers end-to-end haplotigs. Contrarily, Falcon Unzip does not phase the central section, which does not contribute to the total haplotig size.}
% \label{fig:ex_graph_approach}
% \end{figure}
% https://dl.acm.org/citation.cfm?id=322075
% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5411783/
% http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0019175
\section{Further related work}
The assembly problem was initially formalized as the shortest common superstring problem \citep{maier1978complexity} to generate consensus sequence.
The SCS problem turned out to be NP-hard, thus \cite{tarhio1988greedy} followed greedy approach to find approximate solution to solve assembly problem.
The basic idea of this approach is to joining the two reads that overlap the best in terms of overlap length or base quality estimates.
This process is repeated until a predefined minimum quality threshold is reached.

The first genome assemblers \citep{sutton1995tigr, Green99phrapdocumentation} were based on the greedy strategies and have been used during the Human Genome Project.
Greedy approaches were also explored as a possible way to cope with the data generated by the initial set
of second-generation DNA sequencing technologies \citep{warren2006assembling, jeck2007extending}.

The major disadvantage of this approach is that the shortest common superstring problem ignores an important feature of complex (e.g.,
vertebrate) genomes: repeats. This has led to the invention of graph-based models for sequence assembly to handle repeats and other complex genomes.
Graph-based models forms the basis of all modern genome sequence assemblers. 
As introduced in Chapter 1, there are basically two types of graph models such as de Bruijn and OLC-based string graphs.
Over the last few years, a lot of efforts are devoted in improving the graph based assemblers. 

The Edena assembler applied the string graph approach for use with
early short-read sequencing data \citep{hernandez2008novo}. Subsequent theoretical work on efficiently constructing the
string graph using the FM index \citep{simpson2010efficient} led to memory-efficient assemblers for large genomes \citep{li2012exploring, simpson2012efficient}.
Several fast string graph construction algorithms were developed \citep{dinh2011memory, gonnella2012readjoiner, ben2014string}.
 
Several efforts are devoted in the direction of de Bruijn graphs.
The ABySS (Assembly by Short Sequences) assembler \citep{simpson2009abyss} introduced a representation of
the graph that did not explicitly store edges. Rather, it represented the graph as a hash table of
k-mers, with each k-mer storing a byte representing the presence or absence of its eight possible
neighboring k-mers. This representation allowed the hash table to be distributed across a cluster
of computers. In recent years, the use of Bloom filters \citep{bloom1970space} to represent a set of k-mers has gained popularity.
The Bloom filter was first applied to k-mer counting by Melsted & Pritchard \citep{melsted2011efficient}. 
The FM-index data structure developed by Ferragina & Manzini \citep{ferragina2000opportunistic}, which is extensively
used for mapping reads to a reference genome sequence, has also been used for
sequence assembly. An FM index constructed from a set of sequence reads can be queried for the
presence or absence of any k-mer. This representation is independent of k; the FM index can
simultaneously represent all de Bruijn graphs of order k up to the read length.
\cite{ye2012exploiting} observed that only a fraction of k-mers need to be directly stored as vertices. A similar
technique was used to improve the memory consumption of the popular SOAPdenovo assembler \citep{luo2012soapdenovo2}.

Several algorithms have been developed that
explicitly take mate-pair information into account during assembly.
 In \citep{butler2008allpaths, bankevich2012spades}, the assembler attempts to enumerate all the paths connecting the endpoints of a mate pair.
The paired de Bruijn graph \citep{medvedev2011paired} approaches modify the de Bruijn graph
structure to implicitly encode the mate-pair information, thereby resolving segments of the graph
that are consistent with the mate-pair information.
\paragraph{Long-read assemblers}
Although these short-read library preparation methods can extend scaffolds to span entire chromosomes, 
they lack the finer resolution required to improve contig lengths. 
Instead, the biggest gains in contig lengths have come from single-molecule sequencing. First from PacBio and most recently from Oxford Nanopore, these technologies can generate reads exceeding 10 kb, orders of magnitude longer than Illumina. 
Critically, 10-kb reads are longer than the most common repeats in both microbial and vertebrate genomes and can therefore generate highly continuous assemblies. 
In fact, the complete reconstruction of bacterial genomes—a process that used to require teams of people—is now automated and routine. 
However, the massive read lengths and increased error rate of these new technologies have also required updated assembly methods. 
This issue includes new assembly tools designed specifically for long-read PacBio and Nanopore data: Canu \citep{koren2017canu}, HINGE \citep{kamath2017hinge}, Racon \citep{vaser2017fast}, Falcon \citep{chin2016phased}, and Miniasm \citep{li2016minimap}.

Canu, a successor of Celera Assembler that is specifically designed for noisy single-molecule sequences. 
Canu follows a new advanced overlapping and assembly algorithms, including an adaptive overlapping strategy based on tf-idf weighted MinHash and a sparse assembly graph construction that avoids collapsing diverged repeats and haplotypes. 
Racon corrects assemblies by finding a consensus sequence between reads and the assembly through the construction of partial order alignment (POA) graphs. 
After alignment of the reads by a mapper of choice (e.g. Minimap or Graphmap), 
Racon segments the sequence and finds the best alignment between a POA graph of the reads and the assembly. 
By default, the alignment is performed using the Needleman-Wunsch algorithm, which can align sequence and POA graph with little adaptation.
The HGAP (Hierarchical Genome Assembly Process) pipeline
was developed to assemble PacBio-generated data without requiring correction using short-read
data \citep{chin2013nonhybrid}. HGAP is a hierarchical pipeline; it selects the longest PacBio reads to form the basis
of the assembly and error-corrects this subset of reads using the remaining data. The corrected
long reads are then assembled, and a consensus sequence is generated using the complete data
set. This method often generates single-contig assemblies of bacterial genomes.
The FALCON assembler follows the design of the hierarchical genome assembly process (HGAP) but uses more computationally optimized components.
It uses string graph algorithm for genome assembly and best suited for PacBio reads.
It includes several important steps: raw reads error correction, pre-assembly error correction, overlap filtering, construction of graph from overlaps and construction of contig from graph.
\paragraph{Hybrid assemblers}
Aside from mentioned methods, hybrid assembly approaches present another avenue to utilizing nanopore sequencing data. 
\cite{liao2015completing} recently evaluated several assembly tools on PacBio data, including hybrid assemblers SPAdes \cite{bankevich2012spades} and ALLPATHS-LG \citep{gnerre2011high} 
for which they reported good results. Both of these are DBG-based, use Illumina libraries for the primary assembly and then attempt to scaffold the assemblies using longer, less accurate reads. 
Furthermore, SPAdes was recently updated and now officially supports nanopore sequencing data as the long read complement to NGS data. 
Combining single-molecule sequencing with complementary technologies has also become a common strategy. \cite{fan2017hysa} demonstrates improved accuracy for human structural variant calling using a combination of PacBio and Illumina.
\paragraph{Other technologies}
An emerging trend is to combine cost-effective Illumina sequencing with clever library preparation techniques designed to improve assembly continuity. 
One powerful example is chromatin conformation capture via proximity ligation and high-throughput sequencing (Hi-C) \citep{lieberman2009comprehensive}. 
This family of methods generates a familiar paired-read data type (two reads separated by some distance) but from a distribution of sizes that can span megabases. 
This data can be used to group contigs by chromosome, reconstruct chromosome-length scaffolds, and phase haplotypes \citep{burton2013chromosome, selvaraj2013whole}. 
In this issue, \cite{rice2017improved} demonstrates a related approach, using in vitro reconstituted chromatin and Illumina sequencing to assemble the American alligator genome. 
Another approach to boosting short reads uses high-throughput barcoding to tag groups of ``linked reads'' that all originate from a larger, single molecule of DNA. 
For this new data type, \cite{weisenfeld2017direct} introduces a new assembler, Supernova, for the de novo assembly of diploid human genomes from linked reads. 
Additionally, \cite{jackman2017abyss} describes a new version of the ABySS assembler and explores linked reads and optical mapping for improved scaffolding.
\paragraph{Diploid assemblers}
The pipeline for most popular diploid assembler, Falcon Unzip, is given in Figure~\ref{fig:falcon_unzip}.
Falcon Unzip begins by using reads to construct a string graph that contains sets of ``haplotype-fused contigs'' , also called as ``primary contigs'', as well as bubbles representing divergent regions between homologous sequences (Fig.~\ref{fig:falcon_unzip}a). 
Next, Falcon-Unzip identifies read haplotypes using phasing information from heterozygous positions that it identifies (Fig.~\ref{fig:falcon_unzip}b). 
Phased reads are then used to assemble haplotigs and primary contigs (backbone contigs for both haplotypes) (Fig.~\ref{fig:falcon_unzip}c) 
that form the final diploid assembly with phased single-nucleotide polymorphisms (SNPs) and structural variants (SVs).

\textit{Phasing using primary contigs.}
In Falcon Unzip, the reads are aligned to primary contigs and heterozygous SNPs (het-SNPs) are called by analyzing the base frequency of the detailed sequence alignments.
A simple phasing algorithm was developed to identify phased SNPs. 
Along each contig, the algorithm assigns phasing blocks where ``chained phased SNPs'' can be identified. 
Within each block, if a raw read contains a sufficient number of het-SNPs, it assigns a haplotype phase for the read unambiguously. 
Combined with the block and the haplotype phase information, it assigns a ``block-phase'' tag for each phased read in each phasing block.
Some reads might not have enough phasing information. For example, if there are not enough het-SNP sites covered by a read, it assigns a special 'un-phased tag' for each un-phased read.
The initial assembly graph is fused using phased reads and the haplotigs are generated in a greedy manner using local conservative approach.
% \todo{maybe add example how haplotigs from haplotype fused assembly graph works?}

\begin{figure}[t!]\centering
\includegraphics[width=\columnwidth]{{cropped_falcon-unzip}.pdf}
\caption{(a) An initial assembly is computed by FALCON, which error corrects the raw reads (not shown) and then assembles them using a string graph of the read overlaps. 
The assembled contigs are further refined by FALCON-Unzip into a final set of contigs and haplotigs. 
(b) Phase heterozygous SNPs and group reads by haplotype. (c) The phased reads are used to open up the haplotype-fused path and generate as output a set of primary contigs and associated haplotigs.}
\label{fig:falcon_unzip}
\end{figure}



\section{Diploid assembly pipeline}
Our assembly workflow uses short read (e.g.\ Illumina) and long read (e.g.\ PacBio) data combinedly, as illustrated in Figure~\ref{fig:pipeline}, and we describe the details of this process in the following.

\begin{figure}[t!]\centering
\includegraphics[width=\columnwidth]{pipeline.pdf}
\caption{Overview of the diploid assembly pipeline. }
\label{fig:pipeline}
\end{figure}

\subsection{Sequence graph} 
As the first step, we construct a sequence graph using short read data with a low error rate as provided by the Illumina platform.
% The diploid assembly problem can be modelled using a sequence graph (defined below). We construct the underlying sequence graph using Illumina data.
\begin{definition}[Sequence Graph]
We define a sequence graph $G_s (N_s, E_s)$ as a bidirected graph, consisting of a set of nodes $N_s$ and a set of edges $E_s$.
The nodes $n_i$ are sequences over an alphabet $\mathcal{A} = \{A,C,G,T\}$.
For each node $n_i \in N_s$, its reverse-complement is denoted by $n'_i$.
An edge $e_{i'j}$ connects the nodes $n'_i$ to $n_j$. 
Nodes may be traversed in either the forward or reverse direction, with the sequence being reverse-complemented in the reverse direction. 
% \todo{There was a left/right in the definition at some point. Where did it go (and why)?} \sgnote{Richard and I prefer to write in the language of n and n', it is easier and more mathematical. What do you think? TM: That's ok with me. (Although I disagree that it's more mathematical.)}
\end{definition}

In words, edges represent adjacencies between the sequences of the nodes they connect.
Thus, the graph implicitly encodes longer sequences as the concatenated node sequences along walks through the graph.

To illustrate, we consider an example sequence graph $G_s$ in Figure~\ref{fig:wmec}. It consists of a node set $N_s = \{1, 1, 2, 2', 3, 3', \ldots\}$ 
and an edge set $E_s = \{1 \rightarrow 2', 1 \rightarrow 3' \ldots\}$.

To generate the sequence graph $G_s$, we first employ SPAdes \citep{bankevich2012spades}, which constructs a de Bruijn graph and simplifies it, and subsequently remove the overlaps between the nodes in the resulting graph in a process we call \textit{bluntification}.

Overlap information from the assembler is first corrected to account for mismatches between the overlapping sequences. If the nominally overlapping sections of the overlapping nodes are not identical, then the nodes are aligned to each other. If the alignment indicates a perfectly-matching overlap between the appropriate ends of the nodes, then that overlap is used; otherwise, the edge between the nodes is discarded.

After this preprocessing, the bluntification algorithm uses a data structure called a \textit{pinch graph} to transitively resolve overlaps between nodes. Used in the Cactus aligner \citep{paten2011cactus}, a pinch graph is essentially a union-find over oriented positions in a set of DNA sequences. Alternately, it can be thought of as a dynamic representation of a multiple sequence alignment with rearrangements. Two base-pair positions in the sequences that the graph is built on can be \textit{pinched} together, placing them into the same column of the multiple alignment. Bases can be pinched either in consistent orientations (in which case the forward strands of the two positions are aligned) or in opposing orientations (in which case the forward strand of one position is aligned to the reverse strand of the other position). If bases being pinched are already aligned to other bases, the relevant alignment columns are merged; as with a union-find data structure's union operation, pinching is transitive.

Internally, the pinch graph data structure is implemented in terms of contiguous ranges of bases, and so the fundamental operation is to pinch together two equal-length ranges of bases in either consistent or opposing orientations. After a series of pinches, the graph consists of a set of \textit{blocks}, each of which contains a series of oriented \textit{segments}. The segments in a block are contiguous ranges of input sequence bases, and each segment in a block is aligned to all of the other segments in the block.

The bluntification algorithm creates a pinch graph from the nodes that participate in overlaps, pinches together the overlapping regions of all pairs of overlapping nodes, and then converts the resulting pinch graph back into a sequence graph, creating a node for each block.


\subsection{Bubble detection in sequence graph} To account for heterozygosity in a diploid genome, we perform bubble detection. The notion of \textit{bubble} we use is closely based on the \textit{ultrabubble} concept as defined by \cite{paten2017superbubbles}. Briefly, bubbles have the following properties:
\begin{itemize}
 \item \textit{2-node-connectivity.} A bubble is bounded by a fixed start node and an end node. If the graph is traversed in a forward direction, there exists a bubble that starts at a node $n_i$  and ends at a node $n'_j$, then it is given by a start node at $n_j$ and end node at $n'_i$ in a reverse direction. 
 This indicates that a bubble can start and end from either direction.
 \item \textit{Directed acyclicity.} A bubble is directed and acyclic.
 \item \textit{Directionality.} All paths through the bubble flow from start to end.
 \item \textit{Minimality.} No vertex in graph other than a start node $n_i$ (with proper orientation) forms a pair with an end node $n'_j$ (with proper orientation) that satisfy the above properties. 
%  \todo{Reword to avoid ``Only the smallest possible bubble ... is a bubble'' (because larger structures are no bubbles)}
\end{itemize}

A bubble can represent a potential sequencing error or genetic variation within a set of homologous molecules.
We represent bubbles as collections of alternative paths.

\begin{definition}[Path] We define path $a_i$ as a linear ordering of nodes $a_i= n_1, \ldots n_m$. 
% \todo{Why start from a complemented node $n'_1$?}
\label{def:allele-path}
\end{definition}

A bubble is a collection of paths with the same start and end node and can be defined as follows:
\begin{definition}[Bubble]
Formally, one bubble is represented as a collection of allele paths as:
 \[l_k= \{a_1,a_2 \ldots\}\]
 
 where 
 \[a_1=(n_1, n_2, \ldots n_m), a_2=(n_1, n_3 \ldots n_m)\] and so on. 
\end{definition}


For example, Figure~\ref{fig:wmec} shows a set of two bubbles $L=\{l_2, l_1\}$, and  the set of allele paths for a 
% \todo{TM: did you define ``complex bubble'' already?} 
bubble $l_2$ are $\{a_1, a_2, a_3\}$,
where \[a_1 = (6, 7', 8', 11'), a_2 = (6, 9', 11'), a_3=  (6, 10, 11').\]

\subsection{PacBio alignments} 
For phasing bubbles, we consider long reads from third generation sequence technologies such as PacBio.
We align these long reads to the sequence graph $G_s$ to generate paths through the graph.
We perform graph alignment using a banded version of the algorithm described by \cite{rautiainen2017aligning}, which is a generalization of the Needleman-Wunsch algorithm to sequence-to-graph alignment\footnote{\url{https://github.com/maickrau/GraphAligner}}.

There are several advantages of aligning PacBio reads to graphs instead of to a reference genome or contigs.
SNPs often occur near larger variants such as insertions and deletions. SNPs are thus often missed in these regions when reads contain large mismatches. Graph alignment facilitates the alignment to in-phase variants, and other types of complex events.

\begin{definition}[Alignment]
We define a set of read-alignments as $R=\{r_1, r_2, \ldots, r_j\}$, where each read alignment $r_{j'}$ is given by a path of nodes and their orientations through graph $G_s$, written $r_{j'}=(n_1, \ldots, n_m)$.
\end{definition}
For example, in Figure~\ref{fig:wmec}, $R = \{r_1, r_2, r_3, r_4\}$ and the read alignment path $r_1$ can be written as:
\[r_1 = (1, 2', 5', 6, 7', 8', 11' ) \]


\subsection{Bubble ordering}
The next stage is to obtain an ordering of the bubbles $L=(l_1, l_2, \ldots l_k)$, which we refer to as a \emph{bubble chain}. 
For example, in Figure~\ref{fig:wmec}, a bubble chain is given by $L=(l_1, l_2)$.
A general sequence graph $G_s$ is cyclic due to different types of repeats present in the genome that create both short and long cycles.
Ordering bubbles is closely related to resolving repeats, which is a challenging problem.
In this study, we rely on the Canu algorithm \citep{koren2017canu} to provide a bubble ordering by aligning Canu-generated contigs to our sequence graph.
Furthermore, we detect repetitive bubbles---that is, bubbles that would need to be traversed more than once in a final assembly---based on the depth of coverage of aligned PacBio read and remove such bubbles.
% \todo{Shilpa, please check these sentence. I reworded them a bit.}
We deem a bubble repetitive if the number of PacBio reads aligned to its starting node is greater than a coverage threshold specified by users' over the genome.
For example, given a 30$\times$ (=c) data set and a repeat that occurs 20 (=r) times in the genome, then the coverage at the bubble on average is 600 (=rc).

\subsection{Graph-based phasing}
\label{sec:phasing} 
Given a sequence graph $G_s$, ordered bubbles $L$, and PacBio alignments $R$, the goal is to reconstruct two haplotype sequences $\{h_0, h_1\}$, called haplotigs, along each chain of bubbles.
\begin{definition}[Haplotype path]
Formally, a pair of haplotype paths $(h_0, h_1)$ can be defined as two paths through a bubble chain in the sequence graph and denoted as: 
\[h_0=(n_1, n_2, \ldots n_m )\]  
\[h_1=(n_1, n_3, \ldots n_m )\]

where $h_0$ and $h_1$ may differ at the heterozygous regions defined by bubbles, and $n_1$ and $n_m$ are the start and end respectively of the bubble chain.
% \todo{I'm really confused by these $n'$s. I think we can just use plain $n$s.}
\end{definition}

The two genome sequences can be seen as two walks through the bubbles $L$ in the sequence graph $G_s$ that are consistent with the PacBio alignments $R$.
In maximum likelihood terminology, the goal is to find the most likely haplotype paths given the alignment paths traversing through the bubbles.
For example, in Figure~\ref{fig:wmec}, given bubbles $(l_1, l_2)$ and PacBio alignments $R=\{r_1,r_2,r_3,r_4\}$, find two maximum likelihood haplotype paths $\{h_0, h_1\}$ such that each PacBio alignment is assigned to one of the haplotypes. 
% \todo{continue with ``that...'', because you do not just want to find any two paths, but ML ones.}

For a linear chain of bubbles $L$, the task of finding these two haplotype paths is equivalent to picking one allele path per haplotype for each bubble.
% We generate an association between bubbles $L$ and aligned PacBio reads $R$ in the sequence graph $G_s$ in order to encode alignment paths in the form of an allele path in bubbles.
To this end, we note that an alignment path $r_j$ for a given read can be viewed as a sequence of allele paths traversed in consecutive bubbles. 
% stores the information about the allele path $a_i$ in a bubble $l_k$ that the read $r_j$ is aligned.
We represent this association of reads to allele paths in the form of a \emph{bubble matrix} $\mathcal{F}\in\{0,1, \ldots m, -\}^{|R|\times |L|}$, where $|R|$ is the number of reads, $|L|$ is the number of bubbles along a chromosome, and $m = \max_k|l_k|$ is the maximum number of  paths (or alleles) in any bubble $l_k\in L$.
% Please note that the different paths in a bubble corresponds to possible alleles at a genetic variant.
The entry $\mathcal{F}(j,k) \in \{0, 1, \ldots m, -\}$ represents the allele path index in bubble $l_k$ that read $r_j$ is aligned to, where a value of ``$-$'' indicates that the read does not cover the bubble.
% For example, in Figure~\ref{fig:wmec}, the read alignment path $r_1$ traverses an allele path $a_1$ in a bubble $l_1$ and $a_1$ in $l_2$. \todo{TM: The allele paths are not shown in that figure, right? Can we add them?}
In Figure~\ref{fig:wmec}, note that the read alignment path $r_4$ does not cover all the nodes in any of the allele paths in $l_2$ and hence we set the corresponding value $\mathcal{F}(4,2)$ to ``$-$''.
As a result, this read covers only one bubble and we do not consider it for phasing.
The remaining phase-informative reads in Figure~\ref{fig:wmec} are represented as:

\begin{equation}\label{eq:bubble_matrix}
  \mathcal{F}  = \kbordermatrix{
     & l_{1}       & l_{2}  \\
    r_{1}       & 0 & 0 \\
    r_{2}       & 2 & 2 \\
    r_{3}       & 1 & 2 \\
  }
\end{equation}

Corresponding to $\mathcal{F}$, we have a weight matrix $\mathcal{W}\in W^{|R|\times |L|\times m}$. %, where $|R|$ is the number of reads, $|L|$ is the number of bubbles and $m$ is the maximum number of alleles in a bubble $l_k$.
Each entry in $\mathcal{W}(j,k)$ is a tuple to store the ``phred-scaled'' quality scores.
The quality score value ``0'' at $i^\text{th}$ entry in tuple $\mathcal{W}(j,k)$ encodes that the read $r_j$ is aligned to allele path index $i$ in bubble $l_k$.
The remaining non-zero values in tuple $\mathcal{W}(j,k)$ store the confidence scores of switching the aligned read $r_j$ to other alleles in bubble $l_k$.

For example, the corresponding weight matrix $\mathcal{W}(j,k)$ for $\mathcal{F}$~\eqref{eq:bubble_matrix} is given by:
\begin{equation}\label{eq:weight_matrix}
  \mathcal{W}  = \kbordermatrix{
     & l_{1}       & l_{2}  \\
    r_{1}       & [0,q_1,q_2] &  [0,q_3,q_4]\\
    r_2 & [q_9,q_8,0] & [q_{11},q_5,0] \\
    r_3 & [q_{10},0,q_7] & [q_5,q_6,0] \\
  }
\end{equation}
where the entry $\mathcal{W}(1,1)$ value $[0,q_1,q_2]$ means that the read $r_0$ is aligned to allele $a_0$ at bubble $l_1$.
Additionally, the cost of flipping it to other alleles is $q_1$ for $a_1$ and $q_2$ for $a_2$.
% \todo{We should use different $q$s for each entry.}

Finally, we are now ready to present the problem formulation.
The main insight is that solving phasing for bubble chains is similar to solving the phasing problem for multi-allelic SNVs in reference-based haplotype reconstruction.
Therefore, we build on the previous formulation of the Minimum Error Correction (MEC) problem \citep{Lancia2001} and its weighted version (wMEC) \citep{Lippert:2002ba,patterson2014whatshap} and further adapt it to work on a subgraph consisting of a chain of bubbles termed as \emph{Minimum Error Correction for graphs} (gMEC).

\begin{figure}[t!]\centering
\includegraphics[width=\columnwidth]{wmecfig.pdf}
\caption{For a subgraph of $G_s$, the example shows two bubbles $l_1$ and $l_2$, and their corresponding alleles. Reads $r_1,r_2,r_3,r_4$ traverses the bubbles.}
\label{fig:wmec}
\end{figure}

% \begin{definition}[Distance] 
% The quality of a solution relies on the measure $d(r_1,r_2)$ based on the Hamming distance between any two rows $r_1,r_2$. Formally,
% \[d(p_1,p_2):=\sum_{k=1}^{|L|} \big|\big\{k\,\big|\,r_1(k)\neq -\ \wedge\ r_2(k)\neq -\ \wedge\ r_1(k)\neq r_2(k)\big\}\big|.\]
% \end{definition}
% 
% \begin{definition}[Feasibility]
% A feasible solution to a bubble matrix $\mathcal{F}\in\{a_0, a_1 \ldots a_m, -\}^{|R|\times |L|}$ is a pair of haplotypes $h^0,h^1\in\{a_0, a_1, \ldots a_m\}^M$ such that 
% \[d(h_0,h_1):=\sum_{j=1}^{|R|} \min\{ d(\mathcal{F}(j), h_0), d(\mathcal{F}(j), h_1)\} \]
% and there exists a bi-partition of rows (i.\,e., reads) into two sets such that all pairwise distances of two rows within the same set are zero.
% \end{definition}

\begin{problem}[wMEC for bubble chains (gMEC)]
Assume we are given a bubble chain $L$ of bubbles $l_k$, and a set $R$ of reads $r_j$ that pass through these bubbles, with $\mathcal{F}(j,k)$ indicating the index of the allele in bubble $l_k$ that the alignment of read $r_j$ passes through, or ``$-$'' if it does not pass through $l_k$, and that $\mathcal{W}(j,k,i)$ is the cost of flipping $\mathcal{F}(j,k)$ to new value $i$.  
We want to find two paths through $L$, each of which has a non-zero allele value at each $l_k$, and then to flip entries of $\mathcal{F}$ so as to obtain a feasible $\mathcal{F}$, 
i.e. one in which each read is a subsequence of one of the paths, such that we minimize the sum of incurred costs.
\end{problem}
% For simplicity, we represent the $i^{th}$ element of tuple $\mathcal{W}(j,k)$ as $\mathcal{W}(j,k,i)$.
Note that the wMEC problem constitutes a special case of gMEC, where the input graph is a linear chain of bi-allelic bubbles.
% In its linear counterpart, wMEC with each entry in $\mathcal{F}(j,k) \in \{0,1\}$ works for bi-allelic cases.
Next, we describe how to solve gMEC via dynamic programming (DP).

\textit{Solving gMEC for bubble chains}. The basic idea is to now extend the dynamic program to consider all possible path-pairs from a bubble. 
In the bi-allelic case, we have only two paths in every bubble and, therefore, there is only one pair of distinct paths.
In the multi-allelic case, we consider all possible path pairs in each bubble.
The goal is to find an optimal pair of paths from the sequence graph $G_s$.
% \todo{what does ``simplified version'' refer to? After repeat removal?} 
Analogously to the WhatsHap algorithm for wMEC, we proceed from left to right using dynamic programming.  
% Assuming we have completed the process up to bubble $l_k$, at bubble $l_{k+1}$ we consider each possible bipartition of all the reads crossing that bubble into two groups.\todo{TM: unclear wording}
% For each bipartition, we find the best pair of alleles and the best assignment at $l_k$.\todo{Can we be more precise here and mention costs in this column vs.\ costs from previous columns?}
% We continue this way until the last bubble $|L|$ and then backtrace to get the two optimal paths.

Now, we describe how to build the DP table.
Let us take a toy example to understand the algorithm and consider some values in the weight matrix~\eqref{eq:weight_matrix}:
\begin{equation}\label{eq:weight_matrix1}
  \mathcal{W}  = \kbordermatrix{
     & l_{1}       & l_{2}  \\
    r_{1}       & [0,10,5] &  [0,5,8]\\
    r_2 & [7,6,0] & [5,2,0] \\
    r_3 & [2,0,4] & [4,3,0] \\
  }
\end{equation}

\textit{DP cell initialization}. Along similar lines as \cite{patterson2014whatshap}, we first compute the local cost incurred by bipartition $B= (R,S)$ in column $k$, denoted $\Delta_C(k,B)$, and later combine it with the corresponding costs incurred in previous columns.
% We write $\mathcal{S}(k,B)$ to denote this set of sets of reads induced by bipartition $B$ in column $k$.
The cost $W_{k,R}^i$ of flipping all entries in a read set $R$ to an allele index $i\in\{0,1,\ldots |l_k|\}$ is given by 
\[W_{k,R}^i = \sum_{j\in R}\iverl \mathcal{F}(j,k)\neq i\iverr\cdot\mathcal{W}(j,k,i),\]
In the same manner, we can compute costs $W_{k,S}^i$ for read set $S$ to an allele index $i$.

To compute the cost incurred by a bipartition in a particular column $k$, we minimize over all possible pairs of alleles in bubble $l_k$.
There are ${|l_k| \choose 2}$ such pairs.
So given the column vectors of the bubble matrix $\mathcal{F}(k)$, a weight matrix $\mathcal{W}(k)$ of the bubble $l_k$, and the bipartition $B=(R,S)$ of active reads $A(k)$, the cost $\Delta_C(k,B)$ is computed by minimizing over all pairs of alleles
$A = \{(x,y) \in l_k \times l_k | x \ne y, x\textless y\}$:
\begin{equation}\label{eq:dp-cell}
  \Delta_C(k,B)= \min_{(p_0,p_1)\in {A}}\left\{\min\{W_{k,S}^{p_0} + W_{k,R}^{p_1}, W_{k,S}^{p_1} + W_{k,R}^{p_0}\}\right\},
\end{equation}
where the outer minimization considers all allele pairs and the inner minimization considers the two possibilities of assigning those two alleles to the two haplotypes.

   

% \begin{algorithm}
%     \caption{\label{alg:dp-cell}\textsc{DP CELL INITIALIZATION}}
%     \SetKwInOut{Input}{Input}
%     \SetKwInOut{Output}{Output}
%     \Input{The column vectors of the bubble matrix $\mathcal{F}(k)$ and a weight matrix $\mathcal{W}(k)$ of the bubble $k$, and the bipartition $S$ of active reads $R (k)$}
%     \Output{$\Delta_C(k,B)$}
%     All allele-pairs set $A = \{(x,y) \in l_k \times l_k | x \ne y, x\textless y\}$ from bubble $l_k = \{a_1, a_2, \ldots a_m\}$
%     \[\Delta_C(k,B)= \min_{i\in {A}^{\mathcal{S}(k,B)}}\left\{\sum_{S\in\mathcal{S}(k,B)}W_{k,S}^{i}\right\},\]
%     
% \end{algorithm}

\textit{DP column initialization}. 
We initialize the first DP column by setting $C(1,B):=\Delta_C(1,B)$ for all possible bipartitions $B$.
We enumerate all bipartitions in Gray code order, as done previously by \cite{patterson2014whatshap}.
This ensures that only one read is moved from one set to another in each step, facilitating constant time updates of the values $W_{k,S}^i$.

For variant matrix~\eqref{eq:bubble_matrix} and its corresponding weight matrix~\eqref{eq:weight_matrix1}, the DP column cell for bipartition $B=(R,S)$ is given by 
\[\Delta_C(k,(R,S))= \min\big\{\mathcal{W}^0_{k,R} + \mathcal{W}^1_{k,S}, \mathcal{W}^1_{k,R} + \mathcal{W}^2_{k,S}, \]
\[\mathcal{W}^0_{k,R} + \mathcal{W}^2_{k,S}, \mathcal{W}^1_{k,R} + \mathcal{W}^0_{k,S},\]
\[\mathcal{W}^2_{k,R} + \mathcal{W}^1_{k,S}, \mathcal{W}^2_{k,R} + \mathcal{W}^0_{k,S} \big\}\]
Now, putting values from~\eqref{eq:weight_matrix1} in the above equation for different bipartitions, $\Delta_C(1,.)$ can be filled as follows:
\[C(1, (\{r_1,r_2,r_3\},\emptyset)) =\min\{9+0, 16+0, 9+0,16+0, 9+0, 9+0\} = 9\]
% \[C(1, (\{r_1,r_2\},\{r_3\})) = \min\{7+2, 16+0, 5+4, 7+0, 16+4, 7+4\} = 7\]
Similarly, we can compute $\Delta_C(1,.)$ for other bipartitions $(\{r_1,r_2\},\{r_3\}),$\\
$(\{r_1,r_3\},\{r_2\}), (\emptyset,\{r_1,r_2,r_3\}), (\{r_3\},\{r_1,r_2\}), (\{r_2\},\{r_1,r_3\})$.

\begin{algorithm}
    \caption{\label{alg:dp-column}\textsc{DP COLUMN INITIALIZATION}}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{$\Delta_C(k,.)$ for all bipartitions $B$ of bubble $k$.}
    \Output{$C(1,.)$}
    \For{ all bipartitions $B$ of column $k$}{
	Compute $\Delta_C(k,B)$ using Equation~\ref{eq:dp-cell} and store in $C(1,B)$.	
    }
\end{algorithm}
Due to using the Gray code order, we can perform this operation for one DP column in $\mathcal{O}( {|l_k| \choose 2} \cdot 2^{|A(k)|})$ time.

\textit{DP column recurrence}.
Note that $C(k,B)$ is the cost of an optimal solution for input matrices restricted to the first $k$ columns under the constraints that the sought bipartition extends $B$ at bubble $k$.
Entries in column $C(k+1,\cdot)$ should hence add up local costs incurred in column $k+1$ and costs from the previous column (see Algorithm~\ref{alg:dp-table}).
To adhere to the semantics of $C(k+1,B)$, only entries in column $k$ whose bipartitions are \emph{compatible} with $B$ are to be considered as possible ``predecessors'' of $C(k+1, B)$.

For example, consider the second column from~\eqref{eq:bubble_matrix} and~\eqref{eq:weight_matrix1}. Let us compute $C(2,.)$ for different bipartitions using recurrence in Algorithm~\ref{alg:dp-table}:
\[C(2, (\{r_1,r_2,r_3\},\emptyset)) = \min\{9+0, 10+0, 9+0,10+0, 8+0, 8+0\} \]
\[ + \min\{C(1, (\{r_1,r_2,r_3\},\emptyset)\} = 8+9 = 17 \]

To fill DP column $C(2,.)$, we can analogously compute this for the remaining bipartitions $(\{r_1,r_2\},\{r_3\})$,
$(\{r_1,r_3\},\{r_2\})$, $(\emptyset,\{r_1,r_2,r_3\})$, $(\{r_3\},\{r_1,r_2\})$, and $(\{r_2\},\{r_1,r_3\})$.

\begin{algorithm}
    \caption{\label{alg:dp-table}\textsc{DP TABLE}}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{$C(1,.)$ for all bipartitions of bubble $k$.}
    \Output{$C(k,.)$ for all the columns $k$ till last column $|L|$}
    \For{ all columns $k \in \{2 \ldots |L|\}$}{
	Compute $\Delta_C(k+1,B)$ using Equation~\ref{eq:dp-cell}. \\
	Combine it with cost from column $k$ to obtain cost for column $k+1$:
	\[C(k+1,B)= \Delta_C(k+1,B) + \min_{\substack{B'\in\mathcal{B}(A(k)):B'\simeq B\}}}C(k,B')\]
	where $\mathcal{B}\big(A(k)\big)$ denotes the set of all bipartitions of $A(k)$.
    }
\end{algorithm}

\textit{Backtracing.} We can backtrace from the last bubble $C(|L|, \cdot)$ to compute an optimal bipartition $B=(R,S)$ of all input reads.
Given this bipartition, we obtain minimum-cost haplotypes as follows:
Let $P_k=P\cap A(k)$ and $Q_k=Q\cap A(k)$ be an optimal bipartition in column $k$.
We then set
\begin{equation}\label{eqn:haplo_c1}
h_0(k)= a_i \quad \text{with } i:=\argmin_{i'\in \{0,1, \ldots |l_k|\}} W_{k,{P_k}}^{i'}, \nonumber
\end{equation}
\begin{equation}\label{eqn:haplo_c2}
h_1(k)= a_j \quad \text{with } j:=\argmin_{j'\in \{0,1, \ldots |l_k|\}} W_{k,{Q_k}}^{j'}, \nonumber
\end{equation}
where $a_i$ and $a_j$ refer to the corresponding allele paths of bubble $k$ (see Definition~\ref{def:allele-path}).
The full length haplotypes $(h_0, h_1)$ are then obtained by concatenating the bubble paths $\big(h_0(k), h_1(k)\big)$ over all bubbles.

\textit{Time complexity.} 
% As in the previous WhatsHap algorithm \citep{patterson2014whatshap}, we performed algorithm engineering to save the phase information from two consecutive bubbles.
Computing one DP column takes $\mathcal{O}( {m \choose 2} \cdot 2^{|A(k)|})$ time, and the total running time is $\mathcal{O}( {m \choose 2} \cdot 2^{|A(k)|} \cdot |L|)$ for $|L|$ bubbles, where $m$ is the maximum number of alleles in any bubble from $L$. 
It is independent of read-length and, therefore, suitable for increasing read lengths from upcoming technologies.



\subsection{Generation of final assemblies}
We consider the base sequence graph $G_s$ and the two optimal paths $(h_0,h_1)$ to generate final assemblies for every connected component.
For every connected component, we traverse along the haplotype paths $(h_0,h_1)$ and consider the node sequences with their direction information from the base sequence graph $G_s$ in order to generate the final haplotig sequences.

\section{Datasets and experimental setup}
To evaluate the performance of our method, we consider the real data available from two haploid yeast strains SK1 and Y12 \citep{yue2017contrasting}, which we combine to generate a pseudo-diploid yeast.
Both the SK1 and Y12 yeast strains are deeply sequenced using Illumina and PacBio sequencing.
The Illumina dataset is sequenced to an average coverage of 469$\times$ with paired end 151bp reads. We randomly downsample the dataset to a lower average coverage of 50$\times$.
The PacBio data is sequenced to an average coverage of 334$\times$ with an average read length of 4510 bp. 
For coverage analysis, we randomly downsample the PacBio reads to obtain datasets of different coverages $10\times$, $20\times$ and $30\times$ with their average read-lengths of 4482, 4501 and 4516 bp respectively.

\subsection{Pipeline implementation}
\textit{Sequence graph.}
The first step in our pipeline is to clean the data and, therefore, we perform an error correction on the Illumina data by using BFC \citep{li2015bfc}, which, in our experience, retains heterozygosities well for diploid genomes.
BFC is used with default parameters and provided with a genome size of 12.16\,Mbp.
The second step is to generate a sequence graph that includes heterozygosity information.
To construct such a graph, we first construct the assembly graph by using a modified version of SPAdes v3.10.1 \citep{bankevich2012spades} with default parameters plus the \texttt{--only-assembler} option.
It uses the short Illumina reads to generate a De~Bruijn-based assembly graph without any error correction.
We modify the original SPAdes to skip the bubble removal step and retain the heterozygosity information in the graph.
We then convert the assembly graph to a bluntified sequence graph using VG \citep{garrison2017sequence}.
After graph simplification, the resulting sequence graph has 158,567 nodes and 190,767 edges.

\textit{Bubble detection.} In the next stage, we use VG's snarl decomposition algorithm to detect the regions of heterozygosity, or \textit{snarls}, in the sequence graph. This results in 29,071 bubbles.
% and 58,183 total alleles of bubbles. 
% \todo{So there must be bubbles with only one allele since $29095\cdot 2= 58190$. How many multi-allelic bubbles are there?}.

\textit{PacBio Alignments.} After bubble detection, we align the long read PacBio data at different coverages (10$\times$, 20$\times$ and 30$\times$) to the generated sequence graph using GraphAligner\footnote{\url{https://github.com/maickrau/GraphAligner}}.
For efficient alignment of PacBio reads, we use a seed-based approach.
This results in 21,868, 43,459 and 73,129 PacBio alignments for input coverages of $10\times$, $20\times$ and $30\times$, respectively.

\textit{Bubble ordering.} To obtain an ordering of bubbles, we perform \textit{de novo} assembly using Canu v1.5 \citep{koren2017canu} on the PacBio datasets.
As suggested by \cite{giordano2017novo}, we use Canu v1.5 with parameter values as: \texttt{corMhapSensitivity=high, corMinCoverage=2, correctedErrorRate=
0.10, minOverlapLength=499,}\\
\texttt{corMaxEvidenceErate=0.3}. Next, we align these Canu contigs to the sequence graph to obtain the bubble ordering, which we define as the sequence of bubbles encountered by each aligned contig.
Note that we use Canu solely for bubble ordering.
In this paper, we restrict ourselved to phasing bubbles only in the unique regions.
We detect repetitive bubbles based on the coverage depth of the PacBio alignments and remove them from downstream analyses.
This results in 148, 80, 71 bubble chains and in total, 26,576, 27,556 and 27,741 bubbles at coverages of $10\times$, $20\times$, and $30\times$ respectively.
% \todo{What is the definition of ``ordered bubble''? How many bubble chains and how many bubbles per chain on average?} 

\textit{Graph-based phasing.} For each of the coverage conditions, we take as input the ordered bubbles, the long-read PacBio alignments and the sequence graph, and we compute the final haplotigs using gWhatsHap. 
In gWhatsHap, we solve the gMEC problem by assuming constant weights in the weight matrix $\mathcal{W}$.

\textit{True reference alignments.}
To evaluate the accuracy of the predicted haplotypes, we align reference assemblies of the two yeast strains SK1 and Y12 \citep{yue2017contrasting} to the sequence graph.
% In doing so, first, we obtain the true references based on previously generated assemblies of two yeast strains SK1 and Y12 using high coverage PacBio data \citep{yue2017contrasting} and, second, we align these references to the sequence graph.
We emphasize that these reference assemblies are only used for evaluation purposes and are not a part of diploid assembly pipeline.

\subsection{Running Falcon Unzip}
The main goal of this study is to measure the advantages of phasing using a graph based approach, 
and, in particular, the quality of haplotypes at heterozygous sites achievable using the low coverage PacBio data.
Therefore, we compared our graph-based approach to the state-of-the-art contig based phasing method Falcon Unzip, which also generates diploid assemblies.

The Falcon Unzip \citep{chin2016phased} algorithm first constructs a string graph composed of 'haploid consensus' contigs together with bubbles representing structural variant sites between homologous loci. 
Sequenced reads are then phased and separated for each haplotype on the basis of heterozygous positions. 
Phased reads are finally used to assemble the backbone sequence (primary contigs) and the alternative haplotype sequences (haplotigs). 
The combination of primary contigs and haplotigs constitutes the final diploid assembly with phased single-nucleotide polymorphisms and structural variants between the two haplotypes.

We ran Falcon Unzip using the parameters given in the official parameter guide\footnote{http://pb-falcon.readthedocs.io/en/latest/parameters.html}.
Primary contigs and haplotigs were polished using the Quiver algorithm and corrected for SNPs and indels using Illumina data via Pilon with parameters \texttt{--diploid} and \texttt{--fix all} \citep{walker2014pilon}.
To compare the Falcon Unzip haplotigs to the true references, we align the haplotigs produced by Falcon Unzip to our sequence graph and then 
compared these aligned haplotigs to the true reference alignments through the bubbles.

\begin{figure}[t!]\centering
\includegraphics[width=\columnwidth]{evaluation.pdf}
\caption{For a subgraph of $G_s$, the example shows the two true and predicted haplotype alignments (blue and red) through bubbles at the top and bottom. On comparing the respective red and blue lines at the top and botton, we see one switch between SV1 and SV2, which accounts for one switch error. 
The total number of phased bubbles are 5. Therefore, the phasing error rate is 1/5.}
\label{fig:evaluation}
\end{figure}

\section{Assembly performance assessment}
We use the following performance measures for the evaluation of diploid assemblies:

\textit{Phasing error rate}. Over the yeast genome, we compare diploid assemblies with ``true'' haploid assemblies.
For each block, the predicted haplotype is expressed as a mosaic of the two true haplotypes, minimizing the number of switches. 
This minimum is known as the number of switch errors.
% Note that the second predicted haplotype is exactly the complement of the first one, due to only considering heterozygous sites, and so does not have to be considered. \todo{But that's only true for bi-allelic loci, right? What happens at mulit-allellic loci?}
The phasing error rate is defined as the number of switch errors divided by the number of phased bubbles.
Figure~\ref{fig:evaluation} illustrates this calculation.  The top panel shows the true references aligned to the sequence graph. At the bottom, there are predicted haplotypes through the bubbles (using Falcon Unzip or our graph-based approach).
On comparing the true and predicted haplotypes, we see one switch between SV1 and SV2, which accounts for one switch error. The number of phased bubbles are five, the first bubble is considered as phased.
Therefore, the phasing error rate for this example is 1/5.

\textit{Average Percent Identity}. We consider the best assignment of each haplotig to either of the two true references, obtained by aligning the haplotig to the references.
For each whole diploid assembly, we compute the average of the best-alignment percent identities of all the haplotigs. 

\textit{Assembly contiguity.} We assess the contiguity of the assemblies by computing the N50 of haplotig size. 

\textit{Assembly completeness}. We considered two assembly completeness statistics: first, the total length of haplotigs assembled by each method, and second, the total number of unphased contigs.
\begin{center}
\begin{table}
\centering
\begin{tabular}{ |l|c|c|c| } 
 \hline
 Statistics & PacBio & Graph-based  & Falcon Unzip \\ 
 & coverage &  approach &  \\ 
  \hline
  \multicolumn{4}{|c|}{Diploid assemblies Quality}\\
  \hline
 Average Identity[\%] & 10$\times$& 99.50 & \textemdash\\
   & 20$\times$& 99.61  &\textemdash\\
   & 30$\times$& 99.80 &99.4 \\
 Phasing error rate[\%] & 10$\times$& 2.5 & \textemdash\\
   & 20$\times$& 1.5  &\textemdash\\
   & 30$\times$& 0.7 & 3.8 \\
  \hline
  \multicolumn{4}{|c|}{Contiguity}\\
  \hline
   N50 haplotig size [bp]& 10$\times$& 40k &\textemdash\\
   & 20$\times$& 42k &\textemdash\\
   & 30$\times$& 43k &32k\\ 
     \hline
  \multicolumn{4}{|c|}{Completeness}\\
  \hline
  Haplotig size [Mbp] & 10$\times$& 20.7 &\textemdash\\
   & 20$\times$& 21.1 &\textemdash\\
   & 30$\times$& 23.9 &16.6\\
   \# Unphased contigs  & 10$\times$& 2 &\textemdash\\
   & 20$\times$& 2 &\textemdash\\
   & 30$\times$& 2 &77\\
 \hline
\end{tabular}
\\[10pt]
 \caption{Comparison of two phasing methods, Falcon Unzip and our graph-based approach, at different PacBio coverage levels. For computing the ``haplotig N50'', we only consider those portions of a contig for which two haplotypes are available, i.e.\ those regions where Falcon reports a primary contig and an alternative haplotig.
 For ``haplotig size'', we sum the length of contigs on both haplotypes (``primary contigs'' plus ``haplotigs'' in terms of Falcon's output), so the target size is twice the genome size (24.3Mbp in case of yeast).}
\label{table:graph_unzip}
\end{table}
\end{center}

\section{Results}
In this section, we present the results of our analysis of the diploid assemblies generated by our method and by Falcon Unzip on the data sets described above.

\textit{Coverage analysis.} To discover a cost-effective method for assembling a diploid genome, we consider PacBio datasets that vary in terms of coverage---specifically, 10$\times$, 20$\times$ and 30$\times$ coverage are considered.
One of the primary aims of our study is to compare two approaches---the graph-based approach we implemented and the contig-based-phasing Falcon Unzip. In doing so, we quantify the agreement between the diploid assemblies generated by both methods and the true references.
Table \ref{table:graph_unzip} shows the assembly performance statistics for both of these methods.
In order to assess the accuracy of the competing diploid assemblies, we compute the phasing error rate and the average percent identity at different PacBio coverages.
For the graph-based approach, we observe that as we increase the coverage from 10$\times$ to 30$\times$, the average identity of haplotigs increases from 99.5\% to 99.8\% and 
the phasing error rate decreases from 2.5\% to 0.7\%. In contrast, Falcon Unzip, which is not designed for lower coverages, produces haplotigs with an average identity of 99.4\% and phasing error rate of 3.8\% at 30$\times$ coverage.
Overall, comparing the agreement between the graph-based approach (at 10$\times$ coverage) and Falcon Unzip (at 30$\times$ coverage) to the true references, our graph-based approach delivers better haplotigs.
We believe that one reason for this is that we use an Illumina-based graph as a backbone.
Furthermore, solving the gMEC formulation for phasing optimally certainly contributes to generating accurate haplotigs.
Overall, our analysis supports the conclusion that gWhatsHap delivers accurate haplotype sequences even at a long read coverage as low as 10$\times$.

With an increase in average PacBio coverage to 30$\times$, the haplotype contiguity achievable by using our approach improves from 40 kbp to 43 kbp.
By way of comparison, Falcon Unzip delivers haplotigs with a N50 length of 32 kbp at the same coverage level. This highlights the fact that our approach generates more contiguous haplotypes compared to Falcon Unzip.
In terms of haplotypes completeness, our approach yields diploid assemblies of length 20.7 Mbp, 21.1 Mbp and 23.9 Mbp at average PacBio coverages of 10$\times$, 20$\times$ and 30$\times$ respectively.
At coverage 30$\times$, Falcon Unzip delivers a total assembly size of 16.6 Mbp.
% \todo{Does this take the primary contigs into account? I'm still puzzled that this is so low. Did you try higher coverages?}. 
The total length of haplotigs of real pseudo-diploid yeast genome is 24.3 Mbp.
We observe that our approach delivers more complete haplotypes at a low coverage of 10$\times$ compared to Falcon Unzip at a much higher coverage.
There are 2 haplotigs that are not phased by our approach; this is due to the lack of heterozygosity over those regions.
On the other hand, there are 77 (out of 123) contigs that are not phased by Falcon Unzip.
% We believe that the haplotig completeness and contiguity using the graph-based approach is mainly derived from aligning PacBio reads to the sequence graph, and additionally performing bubble calling and phasing them (as illustrated in Figure~\ref{fig:ex_graph_approach}).
In summary, the graph-based approach delivers complete and contiguous haplotype sequences even at a low coverage of 10$\times$.

\textit{Structural variations.} Our next aim is to discover both small and large structural variants phased using the 30$\times$ coverage PacBio dataset.
In Figure~\ref{fig:sv_scatter}, the x-axis represents the edit distance between the two allele (haplotype) paths,
% \todo{I wouldn't call this type and talk of edit distance right away. Confusing otherwise.} 
and the y-axis represents the maximum length from these two haplotype sequences for each bubble.
% The structural variant type for each bubble is determined by calculating the edit distance between the two allele (haplotype) paths, and the maximum variant size is calculated by considering the maximum length from these two haplotype sequences.
The color of each point in the plot indicates the number of corresponding bubbles.
Using gWhatsHap, in Figure~\ref{fig:sv_scatter}, we observe that 26,451 bubbles (out of 29,071) are at a bottom left corner with an edit distance of 1 bp, indicating that the bubbles describe SNVs and other 1 bps indels. 
% shorter variants \todo{The only option are 1bp indels, right? I would say this here.}.
There are a few points that lie on the diagonal (i.e. with one edit per base in the longest allele), which represent larger indels.
% \todo{Also below 20bp, right?}.
The points along the vertical line of the plot represent the bubbles containing variants with a maximum length of 250 and an edit distance of~1~-~4, 
resulting from multiple SNPs or other small variants that are close to each other and that end up being contained in a single bubble.
Furthermore, we see that there are some points above the diagonal, which represent larger and more complex variation.
Overall, the distribution of phased bubbles in the plot indicates that the graph-based approach using both Illumina and PacBio data can phase both small and large structural variations.

% Taken together, the above analysis shows that our graph-based approach delivers better diploid assemblies even at lower PacBio coverages.
% In addition, the graph-based approach helps to detect and phase larger structural variations present in the diploid genomes.

\begin{figure}[t!]\centering
\includegraphics[width=\columnwidth]{wh_bubbles.pdf}
\caption{Structural variation analysis of phased bubbles from our graph-based approach. The x axis and y axis show edit distance and maximum size for the two haplotype paths going through each bubble. The color shows the number of corresponding bubbles.}
\label{fig:sv_scatter}
\end{figure}

% \begin{figure}[t!]\centering
% \includegraphics[width=\columnwidth]{falcon_bubbles.pdf}
% \caption{Structural variation analysis of phased bubbles from Falcon Unzip. The x axis and y axis show edit distance and maximum size for the two haplotype paths going through each bubble. The color shows the density of bubbles.}
% \label{fig:sv_scatter_falcon}
% \end{figure}

\section{Discussion}
The Falcon Unzip method \citep{chin2016phased} is purely based on PacBio data, which exhibits a high error rate, and is therefore not suitable for lower coverages.
By using (costly) high coverage PacBio data, Falcon Unzip is able to generate good quality assemblies with an average haplotig identity of up to 99.99\% \citep{chin2016phased}.
% \todo{I do not see this number in the table above}. %, which is not the cost-effective way to solve the problem.
However, it follows a conservative approach for phasing genomic variants:
As sketched in Figure~\ref{fig:ex_graph_approach}, Falcon Unzip generates long primary contigs, but tends to phase them only partially.
In this example, the central section, which contains non-SNV variants, remains uphased.

To address the above problems, we have provided a novel graph-based approach to diploid genome assembly by combining two different sequencing technologies.
By using one technology producing shorter, more accurate reads, and a second technology delivering long reads, we jointly produce accurate, complete and contiguous haplotypes. 
In doing so, we also provided a cost-effective way of generating high quality assemblies.
Furthermore, by generalizing the diploid assembly problem to sequence graphs, we can detect and phase large structural variants, which was not possible by using linear approaches. 
We have tested our approach using real data for a pseudo-diploid yeast genome, and we have shown that we deliver accurate and complete haplotigs.
Furthermore, we have shown that we can detect and phase structural variants.

In this study, our main focus was to phase unique regions of the genome.
As a next step, we plan look into repetitive regions and phase them as well.
To this end, it is important to note that resolving repeats and polyploid phasing are closely related problems, as pointed out by \cite{Chaisson2017}.
Therefore, we will aim to solve heterozygous variants and repeats in a joint phasing framework, in order to obtain even more contiguous diploid genome assemblies that include both types of features.
That would also remove the need of running an external assembler (Canu) for bubble ordering.
Finally, our framework allows, in principle, to incorporate more data from other sequencing technologies, like chromatin-conformation capture \citep{burton2013chromosome}, linked read sequencing \citep{weisenfeld2017direct}, and single-cell template strand sequencing (Strand-seq; \citealp{Porubsky2016}).
In previous studies on reference-based haplotyping, we have shown such integrative approaches to be very powerful to obtain chromosome-scale haplotypes \citep{porubsky2017dense,chaisson2017multi}, which we believe can be lifted to \textit{de novo} diploid genome assemblies.
Moreover, studying the biological implications of the phased structural variation we can now detect also constitute an exciting future research direction.

